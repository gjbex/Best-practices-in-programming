{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Best practices in programming Material for a training on best practices for programming and software development. Programming languages Although this training aims to be programming language-agnostic, the repository also list a number of tools that are programming language-specific. Obviously, this can not be exhaustive, so feel free to suggest additional tools if you are aware of any. Programming languages covered: C C++ Fortran Python R Table of contents Syntax versus semantics Code style and conventions Version control & collaboration Code documentation Testing Testing as experiments Unit testing Functional testing Code coverage Deployment Continuous integration References","title":"Home"},{"location":"#best-practices-in-programming","text":"Material for a training on best practices for programming and software development.","title":"Best practices in programming"},{"location":"#programming-languages","text":"Although this training aims to be programming language-agnostic, the repository also list a number of tools that are programming language-specific. Obviously, this can not be exhaustive, so feel free to suggest additional tools if you are aware of any. Programming languages covered: C C++ Fortran Python R","title":"Programming languages"},{"location":"#table-of-contents","text":"Syntax versus semantics Code style and conventions Version control & collaboration Code documentation Testing Testing as experiments Unit testing Functional testing Code coverage Deployment Continuous integration References","title":"Table of contents"},{"location":"about/","text":"About This material is licensed under Creative Commons . Acknowledgments I've \"borrowed\" much of the table of contents from a training given by the Netherlands eScience Center, although no actual contents of that training was used for the development of this material.","title":"About"},{"location":"about/#about","text":"This material is licensed under Creative Commons .","title":"About"},{"location":"about/#acknowledgments","text":"I've \"borrowed\" much of the table of contents from a training given by the Netherlands eScience Center, although no actual contents of that training was used for the development of this material.","title":"Acknowledgments"},{"location":"code_style/","text":"Code style A number of very simple things go a long way towards improving your code substantially. For good programmers, they are second nature, and you should strive to make them a habit. This text tries to be programming language-agnostic, and code fragments will be presented in several programming languages such as C, C++, Fortran and Python. However, even if you don't master these languages, the code fragments should be easy enough to understand. Although each programming languages has some specific best practices, many are applicable to any programming language. This is what this text focuses on. We also provide a list of references to best practices specific to various programming languages, and we encourage you strongly to read those as well. In this section, we will use the term function in a very broad sense, simply to keep the text easy to read. In the context of Fortran, a function refers to a program unit, any procedure, either a function or a subroutine. It also refers to a method defined in a class. In the context of Python and C++, we will use the term function for methods as well. Similarly, we use the term variable for constants, and also for attributes of objects and classes, whenever that doesn't lead to confusion. Of course, each programming language has its own style guides, often even several, you can find links to those in the reference section . Format your code nicely To quote Robert C. Martin, \"Code formatting is about communication, and communication is the professional developer\u2019s first order of business\". All programming languages have one or possibly multiple conventions about how to format source code. For example, consistent indentation of code helps considerably to assess the structure of a function at a glance. For multi-level indentation, always use the same width, e.g., multiples of four spaces. The convention you have to use is often determined by the community you are working with, such as your co-workers. It is best to stick to that convention, since coding is communicating. If no convention is established, consider introducing one. The one which is prevalent in the programming language community is most likely to be your best choice. For several programming languages there are tools to automatically format your code so that it adheres to a convention. It is considered good practice to use such tools if they are available, and even to make them part of your development pipeline, either as git pre-commit hooks, or as part of a more substantial CI/CD setup on GitHub or GitLab. Another issue is code formatting is the maximum number of characters on a line. Some guidelines for programmers have very strong opinions on that, while other are more tolerant. Since many editors will automatically display a view on your code with lines wrapped to the length that can be displayed on the screen, that might not seem an issue, but it can be argued that this makes the code harder to read. If you switch that off, you will have a view on your code were some lines are truncated, again a very unpleasant and potentially confusing situation. Personally I like the maximum line length to be 80 characters, since that will guarantee it can be displayed without the need to wrap it, even when displaying multiple editor windows side-by-side. Again, code formatters can enforce this and properly wrap the code for you automatically. Whichever convention you follow, be consistent! Use language idioms Linguists use the term \"idiom\" for an expression that is very specific to a certain language and that cannot be translated literally to another. For instance, the English idiom \"it is raining cats and dogs\" would translate to \"il pleut des cordes\" in French. The corresponding idiom in French is completely unrelated to its counterpart in English. Mastering idioms is one of the requirements for C1 certification, i.e., to be considered to have a proficiency close to that of native speakers. We observe a similar phenomenon for programming languages. Some syntactic constructs are typical for a specific programming language but, when translated one-to-one into another language, lead to code constructs that are unfamiliar to programmers who are proficient in that language. The code fragments below illustrate this for Fortran and C. Although you could write line 4 of the C function below in this way, you most likely wouldn't since it is not idiomatic C. int factorial(int n) { fac = 1; for (int i = 2; i <= n; i++) fac = fac*i; return fac; } The idiomatic formulation of line 4 would be fac *= i . In Fortran for example, you would write real, dimension(10) :: a ... a = value rather than integer :: i real, dimension(10) :: a ... do i = 1, 10 a(i) = value end do Using idioms, i.e., expressions that are particular to a (programming) language, will make your code much easier to interpret correctly by programmers that are fluent in that language. In the spirit of Robert C. Martin's quote: it is all about communication. However, there are sometimes different reasons as well. If we consider the following Python code that uses the numpy library, we can observe a marked performance difference when comparing the two code fragments below. a = np.emptylike(b) for i in range(b.shape[0]): for j in range(b.shape[1]): a[i, j] = np.sqrt(b[i, j]) versus a = np.sqrt(b) The second form is more idiomatic, but it will also substantially outperform the first fragment. MATLAB users will be aware of this difference as well. Idioms for programming languages are usually expressed in code style guidelines. You can find links to those in the reference section . Choose descriptive names In a way, programming is storytelling. The data are the protagonists in the story, and the functions are the actions they take, or what happens to them. Hence variable names should be nouns and functions names should be verbs. If a function returns a property, it should be phrased as a question. Any editor worth its salt provides completion, so you can't argue in favor of short but less descriptive names to save typing. A long but descriptive name is just hitting the tab key away. Choosing descriptive names for variables and functions is another aspect that can make reading your code much easier. Consider the following pseudo-code fragment, and although I'll grant that it is something of a caricature, I've seen some in the wild that are not significantly better. f = open(fn, 'r') for i in f: x = get(i) if condition(x): a = compute(x) if a < 3.14: do_something(a) f.close() A key principle of good software design is that of the least surprise . Choosing appropriate names for our variables and functions helps a lot in this respect. When it comes to programming languages, there is a very clear bias: all those I'm aware of have keywords defined in English. Hence I would argue that code that also uses English variable and function names is easier to read. This saves you from continuously, although unconsciously, to switch from one natural language to another. It may seem convenient, or even more natural to choose variable names in your native language, but that leads to more \"surprises\" when reading the code since you would also expect keywords in your native language to form actual sentences. Moreover, communicating about your code with others that don't speak your language will be just that bit harder. Just to put things in context: sometimes short variable names are perfectly adequate. For instance, trivial variable using in an iteration to index an array are conventionally called i , j and so on. This is perfectly fine, and unless you have a good reason to choose different names, the \"default\" names will even help others to understand your code. Also domain specific variables can have short names, using, e.g., T to denote the temperature, or t for time. This particular example would of course break for programming languages that are case-insensitive, e.g., Fortran. Note that these last remarks in no way contradicts the message of this section: T and t are very descriptive for developers in the domains where this notation is used. Keep it simple Ideally, code is simple. A function should have two levels of indentation at most. This is advice you'll find in the literature on general purpose programming. Although this is good advice, there are some caveats in the context of scientific computing. However, the gist is clear: code is as simple as possible, but not simpler. Even for scientific code, a function has no more lines of code than fit comfortably on your screen. It is all too easy to lose track of the semantics if you can't get an overview of the code. Remember, not everyone has the budget for a 5K monitor. If you find yourself writing a very long code fragment, ask yourself whether that is atomic, or whether the task it represents can be broken up into sub-tasks. If so, and that is very likely, introduce new functions for those sub-tasks with descriptive names. This will make the narrative all the easier to understand. A function should have a single purpose, i.e., you should design it to do one thing, and one thing only. For function signatures, simplicity matters as well. Functions that take many arguments may lead to confusion. In C and C++, you have to remember the order of the function arguments. Accidentally swapping argument values with the same type in a function call can lead to interesting debugging sessions. The same advice applies to Fortran procedures or Python functions, keep the number of arguments limited. However, both Fortran and Python support using keyword arguments, a nice feature that makes your code more robust. Consider the following procedure signature: real function random_gaussian(mu, sigma) implicit none real, intent(in) :: mu, sigma ... end function random_gaussian You would have to check the documentation to know the order of the function arguments. Consider the following four function calls: random_gaussian(0.0, 1.0) : okay; random_gaussian(1.0, 0.0) : not okay; random_gaussian(mu=0.0, sigma=1.0) : okay; random_gaussian(sigma=1.0, mu=0.0) : okay. The two last versions of this call are easier to understand, since the meaning of the numbers is clear. Moreover, since you can use any order, it eliminates a source of bugs. Unfortunately, neither C nor C++ support this feature. Limit scope Many programmers will declare all variables at the start of a block, or even at the start of a function's implementation. This is a syntax requirement in C89 and Fortran. However, C99, C++, R and Python allow you to declare variables anywhere before their first use. Since the scope of a variable starts from its declaration, and extends throughout the block, that means it is in fact too wide. Limiting the scope of declarations to a minimum reduces the probability of inadvertently using the variable, but it also improves code quality: the declaration of the variable is at the same location where the variable is first used, so the narrative is easier to follow. In C++ this may even have performance benefits since a declaration may trigger a call to a potentially expensive constructor. Fortran requires that variables are declared at the start of a compilation unit, i.e., program , function , subroutine , module , but Fortran 2008 introduced the block statement in which local variables can be declared. Their scope doesn't extend beyond the block . Modern compilers support this Fortran 2008 feature. Note that Fortran still allows variables to be implicitly typed, i.e., if you don't declare a variable explicitly, its type will be integer if its starts with the characters i to n , otherwise its type will be real . Consider the code fragment below. Since the variables were not declared explicitly, i is interpreted as integer and total as real . However, the misspelled totl is also implicitly typed as real , initialized to 0.0 , and hence the value of total will be 10.0 when the iterations ends, rather than 100.0 as was intended. integer :: i real :: total do i = 1, 10 total = totl + 10.0 end do To avoid these problems caused by simple typos, use the implicit none statement before variable declarations in program , module , function , subroutine , and block , e.g, implicit none integer :: i real :: total do i = 1, 10 total = totl + 10.0 end do The compiler would give an error for the code fragment above since all variables have to be declared explicitly, and totl was not. Limiting scope of of declarations extends to headers files that are included in C/C++. It is recommended not to include files that are not required. Not only will it pollute the namespace with clutter, but it will also increase build times. This advice is even more important for Python import statements. While the performance impact for C and C++ is limited to compile time, that of unnecessary imports of Python modules will increase the run time of your application. Multithreading When developing multi-threaded C/C++ programs using OpenMP, limiting the scope of variables to parallel regions makes those variables thread-private, hence reducing the risk of data races. We will discuss this in more detail in a later section. Unfortunately, the semantics for the Fortran block statement in an OpenMP do loop is not defined, at least up to the OpenMP 4.5 specification. Although gfortran accepts such code constructs, and seems to generate code with the expected behavior, it should be avoided since Intel Fortran compiler will report an error for such code. This recommendation is mentioned in the C++ core guidelines. Namespaces and imports In C++, you can importing everything defined in a namespace, e.g., using namespace std; Although it saves on typing, it is better to either use the namespace prefix explicitly, or use only what is required, e.g., using std::cout; using std::endl; In Fortran it is also possible to restrict what to use from modules, e.g., use, intrinsic :: iso_fortran_env, only : REAL64, INT32 The only keyword ensures that only the parameters REAL64 and INT32 are imported from the iso_fortran_env module. Note that the intrinsic keyword is used to ensure that the compiler supplied module is used, and not a module with the same name defined by you. Similar advice applies to Python, from math import * is considered bad practice since it pollutes the namespace. Be explicit about constants If a variable's value is not supposed to change during the run time of a program, declare it as a constant, so that the compiler will warn you if you inadvertently modify its value. In C/C++, use the const qualifier, in Fortran, use parameter . If arguments passed to function should be read-only, use const in C/C++ code, and intent(in) in Fortran. Although Fortran doesn't require that you state the intent of arguments passed to procedures, it is nevertheless wise to do so. The compiler will catch at least some programming mistakes if you do. However, this is not quite watertight, in fact, one can still change the value of a variable that is declared as a constant in C. Compile and run the following program, and see what happens. #include <stdio.h> void do_mischief(int *n) { *n = 42; } int main(void) { const int n = 5; printf(\"originally, n = %d\\n\", n); do_mischief((int *) &n); printf(\"mischief accomplished, n = %d\\n\", n); return 0; } In fact, this is explicitly mentioned in the C++ core guidelines. Control access When defining classes in C++ and Fortran, some attention should be paid to accessibility of object attributes. An object's state is determined by its attributes' values, so allowing unrestricted access to these attributes may leave the object in an inconsistent state. In C++, object attributes and methods are private by default, while structure fields and methods are public. For Fortran, fields in user defined types and procedures defined in modules are public by default. Regardless of the defaults, it is useful to specify the access restrictions explicitly. It is good practice to specify private access as the default, and public as the exception to that rule. Interestingly, both Fortran and C++ have the keyword protected , albeit with very different semantics. In Fortran, protected means that a variable defined in a module can be read by the compilation unit that uses it, but not modified. In the module where it is defined, it can be modified though. In C++, an attribute or a method that is declared protected can be accessed from derived classes as well as the class that defines it. However, like attributes and methods declared private , it can not be accessed elsewhere. This is another example where getting confused about the semantics can lead to interesting bugs. In summary: access modifier C++ Fortran private access restricted to class/struct access restricted to module protected access restricted to class/struct and derived variables: modify access restricted to module, read everywhere public attributes and methods can be accessed from everywhere variables, types and procedures can be accessed from everywhere none class: private, struct: public public Python has no notion of private attributes or methods, they are always public. However, attributes and methods that are supposed to be private to the class are by convention prefixed with a _ . Note that this is a convention for programmers, the Python runtime will not enforce this. In both C++ and Python you can \"simulate\" Fortran notion of protected , i.e., read-only attributes by implementing a getter, but no setter. Variable initialization The specifications for Fortran, C and C++ do not define the value an uninitialized variable will have. So you should always initialize variables explicitly, otherwise your code will have undefined, and potentially non-deterministic behavior. When you forget to initialize a variable, the compilers will typically let you get away with it. However, most compilers have optional flags that catch expressions involving uninitialized variables. We will discuss these and other compiler flags in a later section. When initializing or, more generally, assigning a value to a variable that involves constants, your code will be easier to understand when those values indicate the intended type. For example, using 1.0 rather than 1 for floating point is more explicit. This may also avoid needless conversions. This also prevents arithmetic bugs since 1/2 will evaluate to 0 in C, C++ as well as Fortran. Perhaps even more subtly, 1.25 + 1/2 will also evaluate to 1.25 , since the division will be computed using integer values, evaluating to 0 , which is subsequently converted to the floating point value 0.0 , and added to 1.25 . Specifically for C++, I'd strongly encourage you to use universal initialization, since narrowing conversion would lead to warnings. In the code fragment below, the first local variable n1 will be initialized to 7 without any warnings, while the compiler will generate a warning for the initialization of n2 . int conversion(double x) { int n1 = x; int n2 {x}; return n1 + n2; } Precision is also an important factor. If you intend to work purely in single precision for floating point arithmetic operations, it is important to avoid accidental type promotion. For instance, in the code below, the single precision argument is multiplied by 2.1 in ... double precision since that is what the compiler assumes you want to do. When that computation is done, the result is converted back to a single precision value. float times2(float x) { return 2.1*x; } In C and C++, a single precision floating value is denoted by 2.1f to indicate its type. float times2(float x) { return 2.1f*x; } In Fortran, you can similarly make the distinction between kinds of literal numerical values. function times2(x) result(y) implicit none use, intrinsic :: iso_fortran_env, only : sp => REAL32 real(kind=sp), intent(in) :: x real(kind=sp) :: y y = 2.1_sp*x return y end function Similar concerns are important when using numpy in Python. To comment or not to comment? Comments should never be a substitute for code that is easy to understand. In almost all circumstances, if your code requires a comment without which it can not be understood, it can be rewritten to be more clear. Obviously, there are exceptions to this rule. Sometimes we have no alternative but to sacrifice a clean coding style for performance, or we have to add an obscure line of code to prevent a problem caused by over-eager compilers. If you need to add a comment, remember that it should be kept up-to-date with the code. All too often, we come across comments that are no longer accurate because the code has evolved, but the corresponding comment didn't. In such situations, the comment is harmful, since it can confuse us about the intentions of the developer, and at the least, it will cost us time to disambiguate. The best strategy is to make sure that the code tells its own story, and requires no comments. A common abuse of comments is to disable code fragments that are no longer required, but that you still want to preserve. This is bad practice. Such comments make reading the code more difficult, and take up valuable screen real estate. Moreover, when you use a version control system such as git or subversion in your development process, you can delete with impunity, in the sure knowledge that you can easily retrieve previous versions of your files. If you don't use a version control system routinely, you really should. See the additional material section for some pointers to information and tutorials. You should also bear in mind the distinction between comments and documentation. Documentation describes how to use your data types and functions to those who may want to use them. Comments are intended for the consumption of the developers only. You can learn about best practices for documenting your code in the section on documentation . Stick to the standard The official syntax and semantics of languages like C, C++ and Fortran is defined in official specifications. All compilers that claim compliance with these standards have to implement these specifications. However, over the years, compiler developers have added extensions to the specifications. The Intel Fortran compiler for instance has a very long history that can trace its ancestry back to the DEC compiler, and implements quite a number of Fortran extensions. Similarly, the GCC C++ compiler supports some non-standard features. It goes without saying that your code should not rely on such compiler specific extensions, even if that compiler is mainstream and widely available. There is no guarantee that future releases of that same compiler will still support the extension, and the only official information about that extension would be available in the compiler documentation, not always the most convenient source. Moreover, that implies that even if your code compiles with a specific compiler, that doesn't mean it complies with the official language specification. An other compiler would simply generate error message for the same code, and would fail to compile it. Using language extensions makes code harder to read. As a proficient programmer, you're still not necessarily familiar with language extensions, so you may interpret those constructs incorrectly. Hence I'd encourage you strongly to strictly adhere to a specific language specification. For C there are four specifications that are still relevant, C89, C99, C11 and C23. For C++ that would be C++11, C++14, C++17, C++20 and C++23. The relevant specification for Fortran are those of 2003, 2008, and 2018. References to those specifications can be found in the reference section . For C and C++, you may be interested to read the MISRA software development guidelines, a collections of directives and rules specified by the Motor Industry Software Reliability Association (MISRA) aimed at ensuring safer and more reliable software systems in the automotive industry. A reference to this specification is mentioned in the [references][references.md]. The latest and greatest? Programming languages and libraries evolve over time. New features are added, some are deprecated. It is quite important to keep track of the evolution of the programming languages you use. New features are typically added to make your code more robust, or easier to read or write. Similarly, features are deprecated for a reason, usually because they were a Bad Idea(TM), or they can and should be replaced by new ones. In general, it is a good idea to keep up, i.e., start using new features, and especially replace code that is marked as deprecated by your compiler or interpreter. However, don't go over the top. If you use the very latest language features, e.g., the most recent version of Python, latest and greatest C++ standard you should bear in mind that you may cause problems for users of your application or library. It is quite possible that they use systems on which the latest compilers, interpreters and libraries are not available yet, sometimes for very good reasons. With software deployment in mind, you should try to strike a healthy balance between innovation and pragmatism. Maybe it is wiser not to use the latest and greatest feature just yet, but to leave it for the next release? Copy/paste is evil If you find yourself copying and pasting a fragment of code from one file location to another, or from one file to another, you should consider turning it into a function. Apart from making your code easier to understand, it makes it also easier to maintain. Suppose there is a bug in the fragment. If you copy/pasted it, you would have to remember to fix the bug in each instance of that code fragment. If it was encapsulated in a function, you would have to fix the problem in a single spot only. Reuse your code The warning on copy/paste is not only important in the context of a single project, but also across multiple projects. If you find yourself copying functions between projects, it is time to think to redesign your software in a more modular way to facilitate convenient code reuse. Each programming language offers mechanism to develop code in a modular way, e.g., header files in C/C++, modules in Fortran and Python, libraries in R. Structuring common data types and functions into reusable units makes it a lot easier to to reuse that code. In effect, you are building a library of your own. This library forms the core of many of your projects and can be maintained separately. Of course, this comes with a risk if you use your library in multiple projects, say both project A and project B. You have to take care that if you modify the Application Programming Interface (API) or the functionality for project A, it does not break project B. Again, many programming languages make it easy to maintain API compatibility, either through overloading in a language such as C++, or by creative use of optional arguments, allowed by Fortran and Python. For example, suppose you have a function to compute descriptive statistics. Currently, it computes the mean value. def statistics(data): return sum(data)/len(data) In a new project, you would like the function statistics not only to return the mean value, but also the number of data, so you could modify it as shown below. def statistics(data): n = len(data) return sum(data)/n, n Now the function returns a tuple , its first element the mean value, its second the number of elements in data. Although this would of course be fine for the new project, all other projects that rely on the function statistics would break since it would be expected that it returns a float value rather than a tuple . This can be avoided by adding an optional argument as follows. def statistics(data, return_n=False): n = len(data) mean = sum(data)/n return mean, n if return_n else mean Since the return_n argument defaults to False , the function will retain its previous behavior when called in all previous projects, while you can use the added functionality in the new project by calling the function as statistics(data, return_n=True) . Note that such design decisions need to be properly documented. Of course, this approach only works up to a point. It might be necessary to review and redesign the API once in a while to streamline it and reduce the cognitive burden. \"Not invented here\" syndrome Something I observe time after time is the \"not invented here\" syndrome. This means that a developer may prefer to do her own implementation of a function or a class rather than use an existing library developed by others. She may want to avoid the learning curve or the code changes required to use that library, or just doesn't trust a \"black box\". Sometimes that is a wise decision. The overhead incurred by using yet another library to accomplish something that is in fact quite simple to implement yourself may be too high. It will make dependency management and deployment somewhat more complicated . Although package managers are a great help in this respect, tooling is still required, and you may need to update your code if the API of the library changes. On the other hand, re-implementing functionality that already exists in third-party libraries is mostly not a good idea. You are reinventing the wheel, and you might end up with a square one. There may be bugs or performance issues, a lack of flexibility, and it incurs technical debt , i.e., you have to maintain that code for the rest of your software's life cycle. Again, you would have to try to find a healthy balance between implementing functionality yourself and using third-party libraries. Note that this may also have an impact on the license Follow the pattern About half a century into software development, it was recognized that good-quality software often exhibited the same patterns to implement certain features. The Gang of Four (Gamma, Helm, Johnson and Vlissides) published a book that cataloged the patterns they had found, put them into categories and gave both an abstract description and some concrete examples. This book has inspired many others on the same topic, sometimes geared to specific programming languages or application domains. It would lead us too far to go into details about design patters, but it is very useful to familiarize yourself with them so that you can apply them when the situation arises. You will find pointers in the reference section .","title":"Code style"},{"location":"code_style/#code-style","text":"A number of very simple things go a long way towards improving your code substantially. For good programmers, they are second nature, and you should strive to make them a habit. This text tries to be programming language-agnostic, and code fragments will be presented in several programming languages such as C, C++, Fortran and Python. However, even if you don't master these languages, the code fragments should be easy enough to understand. Although each programming languages has some specific best practices, many are applicable to any programming language. This is what this text focuses on. We also provide a list of references to best practices specific to various programming languages, and we encourage you strongly to read those as well. In this section, we will use the term function in a very broad sense, simply to keep the text easy to read. In the context of Fortran, a function refers to a program unit, any procedure, either a function or a subroutine. It also refers to a method defined in a class. In the context of Python and C++, we will use the term function for methods as well. Similarly, we use the term variable for constants, and also for attributes of objects and classes, whenever that doesn't lead to confusion. Of course, each programming language has its own style guides, often even several, you can find links to those in the reference section .","title":"Code style"},{"location":"code_style/#format-your-code-nicely","text":"To quote Robert C. Martin, \"Code formatting is about communication, and communication is the professional developer\u2019s first order of business\". All programming languages have one or possibly multiple conventions about how to format source code. For example, consistent indentation of code helps considerably to assess the structure of a function at a glance. For multi-level indentation, always use the same width, e.g., multiples of four spaces. The convention you have to use is often determined by the community you are working with, such as your co-workers. It is best to stick to that convention, since coding is communicating. If no convention is established, consider introducing one. The one which is prevalent in the programming language community is most likely to be your best choice. For several programming languages there are tools to automatically format your code so that it adheres to a convention. It is considered good practice to use such tools if they are available, and even to make them part of your development pipeline, either as git pre-commit hooks, or as part of a more substantial CI/CD setup on GitHub or GitLab. Another issue is code formatting is the maximum number of characters on a line. Some guidelines for programmers have very strong opinions on that, while other are more tolerant. Since many editors will automatically display a view on your code with lines wrapped to the length that can be displayed on the screen, that might not seem an issue, but it can be argued that this makes the code harder to read. If you switch that off, you will have a view on your code were some lines are truncated, again a very unpleasant and potentially confusing situation. Personally I like the maximum line length to be 80 characters, since that will guarantee it can be displayed without the need to wrap it, even when displaying multiple editor windows side-by-side. Again, code formatters can enforce this and properly wrap the code for you automatically. Whichever convention you follow, be consistent!","title":"Format your code nicely"},{"location":"code_style/#use-language-idioms","text":"Linguists use the term \"idiom\" for an expression that is very specific to a certain language and that cannot be translated literally to another. For instance, the English idiom \"it is raining cats and dogs\" would translate to \"il pleut des cordes\" in French. The corresponding idiom in French is completely unrelated to its counterpart in English. Mastering idioms is one of the requirements for C1 certification, i.e., to be considered to have a proficiency close to that of native speakers. We observe a similar phenomenon for programming languages. Some syntactic constructs are typical for a specific programming language but, when translated one-to-one into another language, lead to code constructs that are unfamiliar to programmers who are proficient in that language. The code fragments below illustrate this for Fortran and C. Although you could write line 4 of the C function below in this way, you most likely wouldn't since it is not idiomatic C. int factorial(int n) { fac = 1; for (int i = 2; i <= n; i++) fac = fac*i; return fac; } The idiomatic formulation of line 4 would be fac *= i . In Fortran for example, you would write real, dimension(10) :: a ... a = value rather than integer :: i real, dimension(10) :: a ... do i = 1, 10 a(i) = value end do Using idioms, i.e., expressions that are particular to a (programming) language, will make your code much easier to interpret correctly by programmers that are fluent in that language. In the spirit of Robert C. Martin's quote: it is all about communication. However, there are sometimes different reasons as well. If we consider the following Python code that uses the numpy library, we can observe a marked performance difference when comparing the two code fragments below. a = np.emptylike(b) for i in range(b.shape[0]): for j in range(b.shape[1]): a[i, j] = np.sqrt(b[i, j]) versus a = np.sqrt(b) The second form is more idiomatic, but it will also substantially outperform the first fragment. MATLAB users will be aware of this difference as well. Idioms for programming languages are usually expressed in code style guidelines. You can find links to those in the reference section .","title":"Use language idioms"},{"location":"code_style/#choose-descriptive-names","text":"In a way, programming is storytelling. The data are the protagonists in the story, and the functions are the actions they take, or what happens to them. Hence variable names should be nouns and functions names should be verbs. If a function returns a property, it should be phrased as a question. Any editor worth its salt provides completion, so you can't argue in favor of short but less descriptive names to save typing. A long but descriptive name is just hitting the tab key away. Choosing descriptive names for variables and functions is another aspect that can make reading your code much easier. Consider the following pseudo-code fragment, and although I'll grant that it is something of a caricature, I've seen some in the wild that are not significantly better. f = open(fn, 'r') for i in f: x = get(i) if condition(x): a = compute(x) if a < 3.14: do_something(a) f.close() A key principle of good software design is that of the least surprise . Choosing appropriate names for our variables and functions helps a lot in this respect. When it comes to programming languages, there is a very clear bias: all those I'm aware of have keywords defined in English. Hence I would argue that code that also uses English variable and function names is easier to read. This saves you from continuously, although unconsciously, to switch from one natural language to another. It may seem convenient, or even more natural to choose variable names in your native language, but that leads to more \"surprises\" when reading the code since you would also expect keywords in your native language to form actual sentences. Moreover, communicating about your code with others that don't speak your language will be just that bit harder. Just to put things in context: sometimes short variable names are perfectly adequate. For instance, trivial variable using in an iteration to index an array are conventionally called i , j and so on. This is perfectly fine, and unless you have a good reason to choose different names, the \"default\" names will even help others to understand your code. Also domain specific variables can have short names, using, e.g., T to denote the temperature, or t for time. This particular example would of course break for programming languages that are case-insensitive, e.g., Fortran. Note that these last remarks in no way contradicts the message of this section: T and t are very descriptive for developers in the domains where this notation is used.","title":"Choose descriptive names"},{"location":"code_style/#keep-it-simple","text":"Ideally, code is simple. A function should have two levels of indentation at most. This is advice you'll find in the literature on general purpose programming. Although this is good advice, there are some caveats in the context of scientific computing. However, the gist is clear: code is as simple as possible, but not simpler. Even for scientific code, a function has no more lines of code than fit comfortably on your screen. It is all too easy to lose track of the semantics if you can't get an overview of the code. Remember, not everyone has the budget for a 5K monitor. If you find yourself writing a very long code fragment, ask yourself whether that is atomic, or whether the task it represents can be broken up into sub-tasks. If so, and that is very likely, introduce new functions for those sub-tasks with descriptive names. This will make the narrative all the easier to understand. A function should have a single purpose, i.e., you should design it to do one thing, and one thing only. For function signatures, simplicity matters as well. Functions that take many arguments may lead to confusion. In C and C++, you have to remember the order of the function arguments. Accidentally swapping argument values with the same type in a function call can lead to interesting debugging sessions. The same advice applies to Fortran procedures or Python functions, keep the number of arguments limited. However, both Fortran and Python support using keyword arguments, a nice feature that makes your code more robust. Consider the following procedure signature: real function random_gaussian(mu, sigma) implicit none real, intent(in) :: mu, sigma ... end function random_gaussian You would have to check the documentation to know the order of the function arguments. Consider the following four function calls: random_gaussian(0.0, 1.0) : okay; random_gaussian(1.0, 0.0) : not okay; random_gaussian(mu=0.0, sigma=1.0) : okay; random_gaussian(sigma=1.0, mu=0.0) : okay. The two last versions of this call are easier to understand, since the meaning of the numbers is clear. Moreover, since you can use any order, it eliminates a source of bugs. Unfortunately, neither C nor C++ support this feature.","title":"Keep it simple"},{"location":"code_style/#limit-scope","text":"Many programmers will declare all variables at the start of a block, or even at the start of a function's implementation. This is a syntax requirement in C89 and Fortran. However, C99, C++, R and Python allow you to declare variables anywhere before their first use. Since the scope of a variable starts from its declaration, and extends throughout the block, that means it is in fact too wide. Limiting the scope of declarations to a minimum reduces the probability of inadvertently using the variable, but it also improves code quality: the declaration of the variable is at the same location where the variable is first used, so the narrative is easier to follow. In C++ this may even have performance benefits since a declaration may trigger a call to a potentially expensive constructor. Fortran requires that variables are declared at the start of a compilation unit, i.e., program , function , subroutine , module , but Fortran 2008 introduced the block statement in which local variables can be declared. Their scope doesn't extend beyond the block . Modern compilers support this Fortran 2008 feature. Note that Fortran still allows variables to be implicitly typed, i.e., if you don't declare a variable explicitly, its type will be integer if its starts with the characters i to n , otherwise its type will be real . Consider the code fragment below. Since the variables were not declared explicitly, i is interpreted as integer and total as real . However, the misspelled totl is also implicitly typed as real , initialized to 0.0 , and hence the value of total will be 10.0 when the iterations ends, rather than 100.0 as was intended. integer :: i real :: total do i = 1, 10 total = totl + 10.0 end do To avoid these problems caused by simple typos, use the implicit none statement before variable declarations in program , module , function , subroutine , and block , e.g, implicit none integer :: i real :: total do i = 1, 10 total = totl + 10.0 end do The compiler would give an error for the code fragment above since all variables have to be declared explicitly, and totl was not. Limiting scope of of declarations extends to headers files that are included in C/C++. It is recommended not to include files that are not required. Not only will it pollute the namespace with clutter, but it will also increase build times. This advice is even more important for Python import statements. While the performance impact for C and C++ is limited to compile time, that of unnecessary imports of Python modules will increase the run time of your application.","title":"Limit scope"},{"location":"code_style/#multithreading","text":"When developing multi-threaded C/C++ programs using OpenMP, limiting the scope of variables to parallel regions makes those variables thread-private, hence reducing the risk of data races. We will discuss this in more detail in a later section. Unfortunately, the semantics for the Fortran block statement in an OpenMP do loop is not defined, at least up to the OpenMP 4.5 specification. Although gfortran accepts such code constructs, and seems to generate code with the expected behavior, it should be avoided since Intel Fortran compiler will report an error for such code. This recommendation is mentioned in the C++ core guidelines.","title":"Multithreading"},{"location":"code_style/#namespaces-and-imports","text":"In C++, you can importing everything defined in a namespace, e.g., using namespace std; Although it saves on typing, it is better to either use the namespace prefix explicitly, or use only what is required, e.g., using std::cout; using std::endl; In Fortran it is also possible to restrict what to use from modules, e.g., use, intrinsic :: iso_fortran_env, only : REAL64, INT32 The only keyword ensures that only the parameters REAL64 and INT32 are imported from the iso_fortran_env module. Note that the intrinsic keyword is used to ensure that the compiler supplied module is used, and not a module with the same name defined by you. Similar advice applies to Python, from math import * is considered bad practice since it pollutes the namespace.","title":"Namespaces and imports"},{"location":"code_style/#be-explicit-about-constants","text":"If a variable's value is not supposed to change during the run time of a program, declare it as a constant, so that the compiler will warn you if you inadvertently modify its value. In C/C++, use the const qualifier, in Fortran, use parameter . If arguments passed to function should be read-only, use const in C/C++ code, and intent(in) in Fortran. Although Fortran doesn't require that you state the intent of arguments passed to procedures, it is nevertheless wise to do so. The compiler will catch at least some programming mistakes if you do. However, this is not quite watertight, in fact, one can still change the value of a variable that is declared as a constant in C. Compile and run the following program, and see what happens. #include <stdio.h> void do_mischief(int *n) { *n = 42; } int main(void) { const int n = 5; printf(\"originally, n = %d\\n\", n); do_mischief((int *) &n); printf(\"mischief accomplished, n = %d\\n\", n); return 0; } In fact, this is explicitly mentioned in the C++ core guidelines.","title":"Be explicit about constants"},{"location":"code_style/#control-access","text":"When defining classes in C++ and Fortran, some attention should be paid to accessibility of object attributes. An object's state is determined by its attributes' values, so allowing unrestricted access to these attributes may leave the object in an inconsistent state. In C++, object attributes and methods are private by default, while structure fields and methods are public. For Fortran, fields in user defined types and procedures defined in modules are public by default. Regardless of the defaults, it is useful to specify the access restrictions explicitly. It is good practice to specify private access as the default, and public as the exception to that rule. Interestingly, both Fortran and C++ have the keyword protected , albeit with very different semantics. In Fortran, protected means that a variable defined in a module can be read by the compilation unit that uses it, but not modified. In the module where it is defined, it can be modified though. In C++, an attribute or a method that is declared protected can be accessed from derived classes as well as the class that defines it. However, like attributes and methods declared private , it can not be accessed elsewhere. This is another example where getting confused about the semantics can lead to interesting bugs. In summary: access modifier C++ Fortran private access restricted to class/struct access restricted to module protected access restricted to class/struct and derived variables: modify access restricted to module, read everywhere public attributes and methods can be accessed from everywhere variables, types and procedures can be accessed from everywhere none class: private, struct: public public Python has no notion of private attributes or methods, they are always public. However, attributes and methods that are supposed to be private to the class are by convention prefixed with a _ . Note that this is a convention for programmers, the Python runtime will not enforce this. In both C++ and Python you can \"simulate\" Fortran notion of protected , i.e., read-only attributes by implementing a getter, but no setter.","title":"Control access"},{"location":"code_style/#variable-initialization","text":"The specifications for Fortran, C and C++ do not define the value an uninitialized variable will have. So you should always initialize variables explicitly, otherwise your code will have undefined, and potentially non-deterministic behavior. When you forget to initialize a variable, the compilers will typically let you get away with it. However, most compilers have optional flags that catch expressions involving uninitialized variables. We will discuss these and other compiler flags in a later section. When initializing or, more generally, assigning a value to a variable that involves constants, your code will be easier to understand when those values indicate the intended type. For example, using 1.0 rather than 1 for floating point is more explicit. This may also avoid needless conversions. This also prevents arithmetic bugs since 1/2 will evaluate to 0 in C, C++ as well as Fortran. Perhaps even more subtly, 1.25 + 1/2 will also evaluate to 1.25 , since the division will be computed using integer values, evaluating to 0 , which is subsequently converted to the floating point value 0.0 , and added to 1.25 . Specifically for C++, I'd strongly encourage you to use universal initialization, since narrowing conversion would lead to warnings. In the code fragment below, the first local variable n1 will be initialized to 7 without any warnings, while the compiler will generate a warning for the initialization of n2 . int conversion(double x) { int n1 = x; int n2 {x}; return n1 + n2; } Precision is also an important factor. If you intend to work purely in single precision for floating point arithmetic operations, it is important to avoid accidental type promotion. For instance, in the code below, the single precision argument is multiplied by 2.1 in ... double precision since that is what the compiler assumes you want to do. When that computation is done, the result is converted back to a single precision value. float times2(float x) { return 2.1*x; } In C and C++, a single precision floating value is denoted by 2.1f to indicate its type. float times2(float x) { return 2.1f*x; } In Fortran, you can similarly make the distinction between kinds of literal numerical values. function times2(x) result(y) implicit none use, intrinsic :: iso_fortran_env, only : sp => REAL32 real(kind=sp), intent(in) :: x real(kind=sp) :: y y = 2.1_sp*x return y end function Similar concerns are important when using numpy in Python.","title":"Variable initialization"},{"location":"code_style/#to-comment-or-not-to-comment","text":"Comments should never be a substitute for code that is easy to understand. In almost all circumstances, if your code requires a comment without which it can not be understood, it can be rewritten to be more clear. Obviously, there are exceptions to this rule. Sometimes we have no alternative but to sacrifice a clean coding style for performance, or we have to add an obscure line of code to prevent a problem caused by over-eager compilers. If you need to add a comment, remember that it should be kept up-to-date with the code. All too often, we come across comments that are no longer accurate because the code has evolved, but the corresponding comment didn't. In such situations, the comment is harmful, since it can confuse us about the intentions of the developer, and at the least, it will cost us time to disambiguate. The best strategy is to make sure that the code tells its own story, and requires no comments. A common abuse of comments is to disable code fragments that are no longer required, but that you still want to preserve. This is bad practice. Such comments make reading the code more difficult, and take up valuable screen real estate. Moreover, when you use a version control system such as git or subversion in your development process, you can delete with impunity, in the sure knowledge that you can easily retrieve previous versions of your files. If you don't use a version control system routinely, you really should. See the additional material section for some pointers to information and tutorials. You should also bear in mind the distinction between comments and documentation. Documentation describes how to use your data types and functions to those who may want to use them. Comments are intended for the consumption of the developers only. You can learn about best practices for documenting your code in the section on documentation .","title":"To comment or not to comment?"},{"location":"code_style/#stick-to-the-standard","text":"The official syntax and semantics of languages like C, C++ and Fortran is defined in official specifications. All compilers that claim compliance with these standards have to implement these specifications. However, over the years, compiler developers have added extensions to the specifications. The Intel Fortran compiler for instance has a very long history that can trace its ancestry back to the DEC compiler, and implements quite a number of Fortran extensions. Similarly, the GCC C++ compiler supports some non-standard features. It goes without saying that your code should not rely on such compiler specific extensions, even if that compiler is mainstream and widely available. There is no guarantee that future releases of that same compiler will still support the extension, and the only official information about that extension would be available in the compiler documentation, not always the most convenient source. Moreover, that implies that even if your code compiles with a specific compiler, that doesn't mean it complies with the official language specification. An other compiler would simply generate error message for the same code, and would fail to compile it. Using language extensions makes code harder to read. As a proficient programmer, you're still not necessarily familiar with language extensions, so you may interpret those constructs incorrectly. Hence I'd encourage you strongly to strictly adhere to a specific language specification. For C there are four specifications that are still relevant, C89, C99, C11 and C23. For C++ that would be C++11, C++14, C++17, C++20 and C++23. The relevant specification for Fortran are those of 2003, 2008, and 2018. References to those specifications can be found in the reference section . For C and C++, you may be interested to read the MISRA software development guidelines, a collections of directives and rules specified by the Motor Industry Software Reliability Association (MISRA) aimed at ensuring safer and more reliable software systems in the automotive industry. A reference to this specification is mentioned in the [references][references.md].","title":"Stick to the standard"},{"location":"code_style/#the-latest-and-greatest","text":"Programming languages and libraries evolve over time. New features are added, some are deprecated. It is quite important to keep track of the evolution of the programming languages you use. New features are typically added to make your code more robust, or easier to read or write. Similarly, features are deprecated for a reason, usually because they were a Bad Idea(TM), or they can and should be replaced by new ones. In general, it is a good idea to keep up, i.e., start using new features, and especially replace code that is marked as deprecated by your compiler or interpreter. However, don't go over the top. If you use the very latest language features, e.g., the most recent version of Python, latest and greatest C++ standard you should bear in mind that you may cause problems for users of your application or library. It is quite possible that they use systems on which the latest compilers, interpreters and libraries are not available yet, sometimes for very good reasons. With software deployment in mind, you should try to strike a healthy balance between innovation and pragmatism. Maybe it is wiser not to use the latest and greatest feature just yet, but to leave it for the next release?","title":"The latest and greatest?"},{"location":"code_style/#copypaste-is-evil","text":"If you find yourself copying and pasting a fragment of code from one file location to another, or from one file to another, you should consider turning it into a function. Apart from making your code easier to understand, it makes it also easier to maintain. Suppose there is a bug in the fragment. If you copy/pasted it, you would have to remember to fix the bug in each instance of that code fragment. If it was encapsulated in a function, you would have to fix the problem in a single spot only.","title":"Copy/paste is evil"},{"location":"code_style/#reuse-your-code","text":"The warning on copy/paste is not only important in the context of a single project, but also across multiple projects. If you find yourself copying functions between projects, it is time to think to redesign your software in a more modular way to facilitate convenient code reuse. Each programming language offers mechanism to develop code in a modular way, e.g., header files in C/C++, modules in Fortran and Python, libraries in R. Structuring common data types and functions into reusable units makes it a lot easier to to reuse that code. In effect, you are building a library of your own. This library forms the core of many of your projects and can be maintained separately. Of course, this comes with a risk if you use your library in multiple projects, say both project A and project B. You have to take care that if you modify the Application Programming Interface (API) or the functionality for project A, it does not break project B. Again, many programming languages make it easy to maintain API compatibility, either through overloading in a language such as C++, or by creative use of optional arguments, allowed by Fortran and Python. For example, suppose you have a function to compute descriptive statistics. Currently, it computes the mean value. def statistics(data): return sum(data)/len(data) In a new project, you would like the function statistics not only to return the mean value, but also the number of data, so you could modify it as shown below. def statistics(data): n = len(data) return sum(data)/n, n Now the function returns a tuple , its first element the mean value, its second the number of elements in data. Although this would of course be fine for the new project, all other projects that rely on the function statistics would break since it would be expected that it returns a float value rather than a tuple . This can be avoided by adding an optional argument as follows. def statistics(data, return_n=False): n = len(data) mean = sum(data)/n return mean, n if return_n else mean Since the return_n argument defaults to False , the function will retain its previous behavior when called in all previous projects, while you can use the added functionality in the new project by calling the function as statistics(data, return_n=True) . Note that such design decisions need to be properly documented. Of course, this approach only works up to a point. It might be necessary to review and redesign the API once in a while to streamline it and reduce the cognitive burden.","title":"Reuse your code"},{"location":"code_style/#not-invented-here-syndrome","text":"Something I observe time after time is the \"not invented here\" syndrome. This means that a developer may prefer to do her own implementation of a function or a class rather than use an existing library developed by others. She may want to avoid the learning curve or the code changes required to use that library, or just doesn't trust a \"black box\". Sometimes that is a wise decision. The overhead incurred by using yet another library to accomplish something that is in fact quite simple to implement yourself may be too high. It will make dependency management and deployment somewhat more complicated . Although package managers are a great help in this respect, tooling is still required, and you may need to update your code if the API of the library changes. On the other hand, re-implementing functionality that already exists in third-party libraries is mostly not a good idea. You are reinventing the wheel, and you might end up with a square one. There may be bugs or performance issues, a lack of flexibility, and it incurs technical debt , i.e., you have to maintain that code for the rest of your software's life cycle. Again, you would have to try to find a healthy balance between implementing functionality yourself and using third-party libraries. Note that this may also have an impact on the license","title":"\"Not invented here\" syndrome"},{"location":"code_style/#follow-the-pattern","text":"About half a century into software development, it was recognized that good-quality software often exhibited the same patterns to implement certain features. The Gang of Four (Gamma, Helm, Johnson and Vlissides) published a book that cataloged the patterns they had found, put them into categories and gave both an abstract description and some concrete examples. This book has inspired many others on the same topic, sometimes geared to specific programming languages or application domains. It would lead us too far to go into details about design patters, but it is very useful to familiarize yourself with them so that you can apply them when the situation arises. You will find pointers in the reference section .","title":"Follow the pattern"},{"location":"continuous_integration/","text":"Continuous integration Continuous Integration (CI) is provided by both GitHub and GitLab. It can be used to make sure that code that is committed is automatically tested and that it can be build, potentially on a matrix of architectures and operating systems. You can find an example of using CI for development in the repository CI-example . A workflow is defined that will run on a push or a pull request to both main and development . If that workflow fails, the pull request can not be merged. The workflow will run pytest and mypy to perform unit tests and static type analysis respectively. This can be a safeguard against accidentally merging commits that break your code. This also relies on the configuration of the main branch that requires the build to succeed in order to allow a merge. The same repository also illustrates how poetry can be used to manage Python development projects. You can also use CI to build your documentation using doxygen, mkdocs or other tools such as, e.g., Sphinx. The documentation can be automatically deployed using GitHub Pages thanks to predefined actions. In fact, these web pages are rendered an published using mkdocs and a GitHub workflow. This workflow will checkout the main branch, render the site using mkdocs in the gh-pages branch so that it is published. Each time a push is done to the main branch the workflow is run, and the latest version is guaranteed to be available via GitHub pages. You can find the workflow definition in the .github/workflows directory in the repository . Since this is in fact a deployment of documentation, it is referred to as Continuous Deployment (CD). It would of course also be possible to typeset a LaTeX document in a workflow, and make a PDF version available for download.","title":"Continuous integration"},{"location":"continuous_integration/#continuous-integration","text":"Continuous Integration (CI) is provided by both GitHub and GitLab. It can be used to make sure that code that is committed is automatically tested and that it can be build, potentially on a matrix of architectures and operating systems. You can find an example of using CI for development in the repository CI-example . A workflow is defined that will run on a push or a pull request to both main and development . If that workflow fails, the pull request can not be merged. The workflow will run pytest and mypy to perform unit tests and static type analysis respectively. This can be a safeguard against accidentally merging commits that break your code. This also relies on the configuration of the main branch that requires the build to succeed in order to allow a merge. The same repository also illustrates how poetry can be used to manage Python development projects. You can also use CI to build your documentation using doxygen, mkdocs or other tools such as, e.g., Sphinx. The documentation can be automatically deployed using GitHub Pages thanks to predefined actions. In fact, these web pages are rendered an published using mkdocs and a GitHub workflow. This workflow will checkout the main branch, render the site using mkdocs in the gh-pages branch so that it is published. Each time a push is done to the main branch the workflow is run, and the latest version is guaranteed to be available via GitHub pages. You can find the workflow definition in the .github/workflows directory in the repository . Since this is in fact a deployment of documentation, it is referred to as Continuous Deployment (CD). It would of course also be possible to typeset a LaTeX document in a workflow, and make a PDF version available for download.","title":"Continuous integration"},{"location":"deployment/","text":"Deployment Build tools Making sure you software can be built and installed easily is a very important part of the software's life cycle. It will be a hard requirement if you intend your software to be adopted and used by others. For compiled languages such as C, C++ and Fortran you will need to set up how to compile and link your code. Several build environment are available such as autotools and CMake. Although scripting languages such as Python and R don't require compilation you would still rely on these environments if you build extensions for these languages. Although it takes time to properly configure the build process, and there is a learning curve, it will save you a lot of time in the long run. Most build environments also allow you to run tests, and that includes those for scripting languages. Integrating test execution as part of your build process is most certainly a best practice. Deployment Most build environments let you easily create distributions of your software projects, for instance under the form of source archives that include the build configuration files. Some environments go even a step further by supporting uploads to software distribution repositories. The build systems can also be configured to do the installation of software, something that can be tuned further by the person who installs the software on her own system. Package managers Many software project rely on packages that need to be installed to successfully link or run the application or library. Managing these dependencies can be pretty tedious. Some build tools will help you to easily integrate such libraries by configuring the build configuration with the information on where they are installed on a system. Most build systems also support a form a dependency management or package management that allows you to easily download and install packages. Typically these installs will be local to the project, so it is not necessary to install them centrally and \"pollute\" your system. This dependency isolation makes it easier to test the software build system and makes it more portable to other computers and operating systems. License A very important aspect of software deployment is licensing. Releasing software without a license or with one that is not appropriate can have dire consequences. It is also very important to verify that the licenses of software component or tools are compatible with the one you select for yourself. There is a nice website that helps you select an appropriate license . However, when in doubt, get in touch with your Technology Transfer Office, they will be able to advice. Can I cite you on that? If you choose a license that enforces those that use it to give you credit, it makes sense to make it as easy as possible for them to do so. The same applies if you hope to be cited in scientific publications that use your software. You can associate a Digital Object Identifier (DOI) with your GitHub repository. This service is offered by Zenodo that integrates well with GitHub. When you do a release, your repository gets a DOI that you can include in your README.md as a badge. This is the case for the GitHub repository for this training material. Examples Examples of using various tools can be found in other repositories. CMake use cases Poetry C/C++ package managers","title":"Deployment"},{"location":"deployment/#deployment","text":"","title":"Deployment"},{"location":"deployment/#build-tools","text":"Making sure you software can be built and installed easily is a very important part of the software's life cycle. It will be a hard requirement if you intend your software to be adopted and used by others. For compiled languages such as C, C++ and Fortran you will need to set up how to compile and link your code. Several build environment are available such as autotools and CMake. Although scripting languages such as Python and R don't require compilation you would still rely on these environments if you build extensions for these languages. Although it takes time to properly configure the build process, and there is a learning curve, it will save you a lot of time in the long run. Most build environments also allow you to run tests, and that includes those for scripting languages. Integrating test execution as part of your build process is most certainly a best practice.","title":"Build tools"},{"location":"deployment/#deployment_1","text":"Most build environments let you easily create distributions of your software projects, for instance under the form of source archives that include the build configuration files. Some environments go even a step further by supporting uploads to software distribution repositories. The build systems can also be configured to do the installation of software, something that can be tuned further by the person who installs the software on her own system.","title":"Deployment"},{"location":"deployment/#package-managers","text":"Many software project rely on packages that need to be installed to successfully link or run the application or library. Managing these dependencies can be pretty tedious. Some build tools will help you to easily integrate such libraries by configuring the build configuration with the information on where they are installed on a system. Most build systems also support a form a dependency management or package management that allows you to easily download and install packages. Typically these installs will be local to the project, so it is not necessary to install them centrally and \"pollute\" your system. This dependency isolation makes it easier to test the software build system and makes it more portable to other computers and operating systems.","title":"Package managers"},{"location":"deployment/#license","text":"A very important aspect of software deployment is licensing. Releasing software without a license or with one that is not appropriate can have dire consequences. It is also very important to verify that the licenses of software component or tools are compatible with the one you select for yourself. There is a nice website that helps you select an appropriate license . However, when in doubt, get in touch with your Technology Transfer Office, they will be able to advice.","title":"License"},{"location":"deployment/#can-i-cite-you-on-that","text":"If you choose a license that enforces those that use it to give you credit, it makes sense to make it as easy as possible for them to do so. The same applies if you hope to be cited in scientific publications that use your software. You can associate a Digital Object Identifier (DOI) with your GitHub repository. This service is offered by Zenodo that integrates well with GitHub. When you do a release, your repository gets a DOI that you can include in your README.md as a badge. This is the case for the GitHub repository for this training material.","title":"Can I cite you on that?"},{"location":"deployment/#examples","text":"Examples of using various tools can be found in other repositories. CMake use cases Poetry C/C++ package managers","title":"Examples"},{"location":"documentation/","text":"Documentation best practices When you consider using a new software library, you probably like to start by looking at some example code. You'll do a few experiments of your own, based on those examples. Once you start using the new library for non-trivial applications, you will most likely have to refer to the reference guide. All this documentation is a great help if its quality is good. If you want to deliver high quality software, the documentation is a very important aspect of the development process. You can distinguish between two types of documentation: tutorial style and reference documentation. Types of documentation Broadly speaking, we distinguish two types of documentation, tutorials and reference guides. They serve different purposes, and hence are complementary. Tutorials will typically illustrate a number of use cases, presented as a narrative. It is more of a high-level overview of the software project. Such documentation is typically written separately from the source code, and a convenient format is markdown. We will discuss MkDocs, a tools to generate nice looking documentation based on this format. A reference guide serves a different purpose. It is intended to get low-level information on the contents of the library. It should be easy to navigate, so that one can view the definition of a datatype by clicking it when it occurs in the signature of a function. Hence this type of documentation integrates closely with the source code itself. If the latter changes, so should the former. Pitfalls When working on a software project, one of the main pitfalls is neglecting documentation. Many developers don't particularly enjoy writing documentation, and hence postpone it. The more classes and functions to be documented, the worse the tasks seems to be, and the larger the probability that it will never happen. Getting into the habit of immediately writing the reference documentation for each module, data type or function as you code is certainly a \"best practice\". Although it may seem to slow you down while coding, that is not necessarily a bad thing. However, even when documentation is diligently written while coding, it has to be kept up to date when code changes. It is all too easy to forget to update the documentation when the signature of a function is modified. Inaccurate documentation is almost worse than no documentation since it is likely to cause serious software defects. What to document? The first question to answer is, what do you need to document since a software project has many artifacts. Documenting functions Consider the documentation of a function or a subroutine for instance. You would write a description of what the it does, which argument it takes, and what the return type is. However, you would also add the assumptions the procedure makes about the values its arguments can have. Such assumptions are called preconditions. By way of example, consider a function that takes a temperature as an argument. An important aspect is the units it expects the temperature to be specified in. Will that be degrees Celsius, or Fahrenheit, or Kelvin? In case it is degrees Celsius, it doesn't make sense to pass a value to the function that is less than -273.15 degrees Celsius. On the other hand, if it is Kelvin, the value should be positive. Needless to say that if you pass a temperature expressed in Kelvin to a function that expects degrees Celsius as a unit, the results will be wrong. This example illustrates an important point: the function documentation should clearly specify its expectation about the arguments that you pass to it. In our example, that would be the units of the temperature, but also the valid range of values. These restrictions are often called preconditions, and they constitute a contract between the user of the procedure and its author. The contract, i.e., the precondition, says that if the user of the function supplies arguments that fulfill all the preconditions, then the author guarantees that the function will return proper results. When considering values returned by a function, you have to consider similar restrictions. What are the units of the return value and what is the range of values that would make sense? This amounts to postconditions, again a contract between the function's author and its user. The contract, i.e., the postcondition, specifies that the user of the function can rely on it to return a value that fulfills all the conditions if she calls it correctly. When a procedure has side effects, i.e., it modifies one of its arguments, this should be stated as well. The \"principle of least astonishment\" is very useful in software development. Conversely, when passing an argument to a function that is not changed at all, that constitutes an invariant. Although it may be useful to state this in the documentation, it is far better practice to make that explicit by declaring it constant. The concepts of preconditions, postconditions, and invariants were introduced in the \"design by contract\" paradigm, pioneered by Bertrand Meyer in 1986. If you make it part of your coding process to immediately provide the documentation when writing a procedure, it will actually help you since you have to make your assumptions explicit, and hence may discover potential flaws. Documenting failure is very important as well. Can the function generate an error? What is the semantics of these errors? Again, this is an application of the \"principle of least astonishment\". The user of your functions will be aware of potential failure, and can code to deal with it appropriately. Documenting data types For data types, the semantics of the type should be documented. What does a variable of the type represent? Of course, the type name should be chosen so that this is clear, but it doesn't hurt to explain this more formally in the documentation as well. For each field, you should document its type, semantics, and, if applicable, the units that are expected. When doing object oriented programming, the documentation of methods is similar to that of functions. Documenting modules Code is typically aggregated according to functionality in modules for separate compilation or importing. Hence this aggregation has a purpose, and you should document that as well. What is the overall purpose of the module? Another question that you can answer in module documentation is the interaction between various components. How can the output of one function be used as the input for another, are the various functions independent, or should they be called in a certain order? As an example, if you have a function that initializes a data structure, a few that manipulate such data structures, and a finalization function that releases the resources in that data structure, then the module documentation should probably mention that you should call the initialization function to get a valid data structure; call functions that use the data structure; and call the finalization function to avoid memory leaks. At this level, you may want to add some example code of using the software components in that module. How do you call the procedures, how can you use the return values, what are the use cases? This type of documentation could also be maintained at the project level, though. Documenting projects This is typically high-level documentation in the form of a tutorial with use cases and code samples. It should of course contain a link to the reference documentation.","title":"Documentation"},{"location":"documentation/#documentation-best-practices","text":"When you consider using a new software library, you probably like to start by looking at some example code. You'll do a few experiments of your own, based on those examples. Once you start using the new library for non-trivial applications, you will most likely have to refer to the reference guide. All this documentation is a great help if its quality is good. If you want to deliver high quality software, the documentation is a very important aspect of the development process. You can distinguish between two types of documentation: tutorial style and reference documentation.","title":"Documentation best practices"},{"location":"documentation/#types-of-documentation","text":"Broadly speaking, we distinguish two types of documentation, tutorials and reference guides. They serve different purposes, and hence are complementary. Tutorials will typically illustrate a number of use cases, presented as a narrative. It is more of a high-level overview of the software project. Such documentation is typically written separately from the source code, and a convenient format is markdown. We will discuss MkDocs, a tools to generate nice looking documentation based on this format. A reference guide serves a different purpose. It is intended to get low-level information on the contents of the library. It should be easy to navigate, so that one can view the definition of a datatype by clicking it when it occurs in the signature of a function. Hence this type of documentation integrates closely with the source code itself. If the latter changes, so should the former.","title":"Types of documentation"},{"location":"documentation/#pitfalls","text":"When working on a software project, one of the main pitfalls is neglecting documentation. Many developers don't particularly enjoy writing documentation, and hence postpone it. The more classes and functions to be documented, the worse the tasks seems to be, and the larger the probability that it will never happen. Getting into the habit of immediately writing the reference documentation for each module, data type or function as you code is certainly a \"best practice\". Although it may seem to slow you down while coding, that is not necessarily a bad thing. However, even when documentation is diligently written while coding, it has to be kept up to date when code changes. It is all too easy to forget to update the documentation when the signature of a function is modified. Inaccurate documentation is almost worse than no documentation since it is likely to cause serious software defects.","title":"Pitfalls"},{"location":"documentation/#what-to-document","text":"The first question to answer is, what do you need to document since a software project has many artifacts.","title":"What to document?"},{"location":"documentation/#documenting-functions","text":"Consider the documentation of a function or a subroutine for instance. You would write a description of what the it does, which argument it takes, and what the return type is. However, you would also add the assumptions the procedure makes about the values its arguments can have. Such assumptions are called preconditions. By way of example, consider a function that takes a temperature as an argument. An important aspect is the units it expects the temperature to be specified in. Will that be degrees Celsius, or Fahrenheit, or Kelvin? In case it is degrees Celsius, it doesn't make sense to pass a value to the function that is less than -273.15 degrees Celsius. On the other hand, if it is Kelvin, the value should be positive. Needless to say that if you pass a temperature expressed in Kelvin to a function that expects degrees Celsius as a unit, the results will be wrong. This example illustrates an important point: the function documentation should clearly specify its expectation about the arguments that you pass to it. In our example, that would be the units of the temperature, but also the valid range of values. These restrictions are often called preconditions, and they constitute a contract between the user of the procedure and its author. The contract, i.e., the precondition, says that if the user of the function supplies arguments that fulfill all the preconditions, then the author guarantees that the function will return proper results. When considering values returned by a function, you have to consider similar restrictions. What are the units of the return value and what is the range of values that would make sense? This amounts to postconditions, again a contract between the function's author and its user. The contract, i.e., the postcondition, specifies that the user of the function can rely on it to return a value that fulfills all the conditions if she calls it correctly. When a procedure has side effects, i.e., it modifies one of its arguments, this should be stated as well. The \"principle of least astonishment\" is very useful in software development. Conversely, when passing an argument to a function that is not changed at all, that constitutes an invariant. Although it may be useful to state this in the documentation, it is far better practice to make that explicit by declaring it constant. The concepts of preconditions, postconditions, and invariants were introduced in the \"design by contract\" paradigm, pioneered by Bertrand Meyer in 1986. If you make it part of your coding process to immediately provide the documentation when writing a procedure, it will actually help you since you have to make your assumptions explicit, and hence may discover potential flaws. Documenting failure is very important as well. Can the function generate an error? What is the semantics of these errors? Again, this is an application of the \"principle of least astonishment\". The user of your functions will be aware of potential failure, and can code to deal with it appropriately.","title":"Documenting functions"},{"location":"documentation/#documenting-data-types","text":"For data types, the semantics of the type should be documented. What does a variable of the type represent? Of course, the type name should be chosen so that this is clear, but it doesn't hurt to explain this more formally in the documentation as well. For each field, you should document its type, semantics, and, if applicable, the units that are expected. When doing object oriented programming, the documentation of methods is similar to that of functions.","title":"Documenting data types"},{"location":"documentation/#documenting-modules","text":"Code is typically aggregated according to functionality in modules for separate compilation or importing. Hence this aggregation has a purpose, and you should document that as well. What is the overall purpose of the module? Another question that you can answer in module documentation is the interaction between various components. How can the output of one function be used as the input for another, are the various functions independent, or should they be called in a certain order? As an example, if you have a function that initializes a data structure, a few that manipulate such data structures, and a finalization function that releases the resources in that data structure, then the module documentation should probably mention that you should call the initialization function to get a valid data structure; call functions that use the data structure; and call the finalization function to avoid memory leaks. At this level, you may want to add some example code of using the software components in that module. How do you call the procedures, how can you use the return values, what are the use cases? This type of documentation could also be maintained at the project level, though.","title":"Documenting modules"},{"location":"documentation/#documenting-projects","text":"This is typically high-level documentation in the form of a tutorial with use cases and code samples. It should of course contain a link to the reference documentation.","title":"Documenting projects"},{"location":"license/","text":"Attribution 4.0 International ======================================================================= Creative Commons Corporation (\"Creative Commons\") is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \"as-is\" basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible. Using Creative Commons Public Licenses Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses. Considerations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC- licensed material, or material used under an exception or limitation to copyright. More considerations for licensors: wiki.creativecommons.org/Considerations_for_licensors Considerations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor's permission is not necessary for any reason--for example, because of any applicable exception or limitation to copyright--then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More_considerations for the public: wiki.creativecommons.org/Considerations_for_licensees ======================================================================= Creative Commons Attribution 4.0 International Public License By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions. Section 1 -- Definitions. a. Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image. b. Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License. c. Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights. d. Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements. e. Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material. f. Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License. g. Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license. h. Licensor means the individual(s) or entity(ies) granting rights under this Public License. i. Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them. j. Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world. k. You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning. Section 2 -- Scope. a. License grant. 1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to: a. reproduce and Share the Licensed Material, in whole or in part; and b. produce, reproduce, and Share Adapted Material. 2. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions. 3. Term. The term of this Public License is specified in Section 6(a). 4. Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a) (4) never produces Adapted Material. 5. Downstream recipients. a. Offer from the Licensor -- Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License. b. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material. 6. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i). b. Other rights. 1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise. 2. Patent and trademark rights are not licensed under this Public License. 3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties. Section 3 -- License Conditions. Your exercise of the Licensed Rights is expressly made subject to the following conditions. a. Attribution. 1. If You Share the Licensed Material (including in modified form), You must: a. retain the following if it is supplied by the Licensor with the Licensed Material: i. identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated); ii. a copyright notice; iii. a notice that refers to this Public License; iv. a notice that refers to the disclaimer of warranties; v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable; b. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and c. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License. 2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information. 3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable. 4. If You Share Adapted Material You produce, the Adapter's License You apply must not prevent recipients of the Adapted Material from complying with this Public License. Section 4 -- Sui Generis Database Rights. Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material: a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database; b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database. For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights. Section 5 -- Disclaimer of Warranties and Limitation of Liability. a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU. b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU. c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability. Section 6 -- Term and Termination. a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically. b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates: 1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or 2. upon express reinstatement by the Licensor. For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License. c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License. d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License. Section 7 -- Other Terms and Conditions. a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed. b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License. Section 8 -- Interpretation. a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License. b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions. c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor. d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority. ======================================================================= Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \u201cLicensor.\u201d The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark \"Creative Commons\" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses. Creative Commons may be contacted at creativecommons.org.","title":"License"},{"location":"references/","text":"References Style guides Style guides for various programming languages. C MISRA C2023 C++ C++ core guidelines Google C++ style guide MISRA C++2023 Fortran Fortran style guide Norman S. Clerman and Walter Spector, \"Modern Fortran: style and usage\" , Cambridge University Press, 2011 Python PEP8 Python style guide R tidyverse style guide Programming language standards C standards C++ standards Fortran standards Programming languages such as Python and R don't have formal standards. General best practices General resources that discuss best practices in software engineering. Brain Kernighan and Rob Pike, \"The practice of programming\" , Addison-Wesley, 1999 Martin Fowler, \"Refactoring: Improving the Design of Existing Code\" , Addison Wesley, 1999 Erich Gamma, Richard Helm, Ralpha Johnson and John Vlissides, \"Design patterns: elements of reusable object-oriented software\" , Addison-Wesley, 1994 Robert C. Martin, \"Clean code: a handbook of agile software craftsmanship\" , Pearson, 2008 Software licenses Discussion of how to choose an appropriate license on the GNU website. Scientific software Some references that are more specific to scientific software development. Task Force Sub Group 3 - Review of Software Quality Attributes and Characteristics","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"references/#style-guides","text":"Style guides for various programming languages.","title":"Style guides"},{"location":"references/#c","text":"MISRA C2023","title":"C"},{"location":"references/#c_1","text":"C++ core guidelines Google C++ style guide MISRA C++2023","title":"C++"},{"location":"references/#fortran","text":"Fortran style guide Norman S. Clerman and Walter Spector, \"Modern Fortran: style and usage\" , Cambridge University Press, 2011","title":"Fortran"},{"location":"references/#python","text":"PEP8 Python style guide","title":"Python"},{"location":"references/#r","text":"tidyverse style guide","title":"R"},{"location":"references/#programming-language-standards","text":"C standards C++ standards Fortran standards Programming languages such as Python and R don't have formal standards.","title":"Programming language standards"},{"location":"references/#general-best-practices","text":"General resources that discuss best practices in software engineering. Brain Kernighan and Rob Pike, \"The practice of programming\" , Addison-Wesley, 1999 Martin Fowler, \"Refactoring: Improving the Design of Existing Code\" , Addison Wesley, 1999 Erich Gamma, Richard Helm, Ralpha Johnson and John Vlissides, \"Design patterns: elements of reusable object-oriented software\" , Addison-Wesley, 1994 Robert C. Martin, \"Clean code: a handbook of agile software craftsmanship\" , Pearson, 2008","title":"General best practices"},{"location":"references/#software-licenses","text":"Discussion of how to choose an appropriate license on the GNU website.","title":"Software licenses"},{"location":"references/#scientific-software","text":"Some references that are more specific to scientific software development. Task Force Sub Group 3 - Review of Software Quality Attributes and Characteristics","title":"Scientific software"},{"location":"syntax_vs_semantics/","text":"Syntax versus semantics \"Syntax\" and \"semantics\" are two terms we will use often in this training. Most of you are probably familiar with these concepts, but just to make sure, let's define them briefly. The syntax of a programming language is its grammar, i.e., the rules the source code text must satisfy in order to be considered syntactically correct. Let's illustrate that with an example from natural language. The sentence \"the dog barks\" is syntactically correct, while \"the dog bark\" is not. Semantics on the other hand has to do with meaning, or interpretation. Again drawing on natural language examples, the sentence \"the dog spoke\" is syntactically correct, but the semantics is wrong. Obviously, dogs do not speak, except in fairy tales. The sentence \"the dog barks\" is both syntactically and semantically correct. The following code fragments would be syntactically incorrect. program syntax_error implicit none use, intrinsic :: iso_fortran_env, only : output_unit write (unit=output_unit, fmt='(A)') 'hello world' end program syntax_error This Fortran program has a syntax error that is dutifully reported by the gfortran compiler as: ~~~bash $ gfortran -c syntax_error.f90 syntax_error.f90:3.57: use, intrinsic :: iso_fortran_env, only : output_unit 1 syntax_error.f90:2.17: implicit none 2 Error: USE statement at (1) cannot follow IMPLICIT NONE statement at (2) The following Fortran function definition has a semantic error, although it is syntactically correct. ~~~~fortran integer function fac(n) implicit none integer, intent(IN) :: n integer :: i fac = 1 do i = 0, n fac = fac*i end do end function fac The code fragment above will be compiled without errors or even warnings, but the function will return zero when invoked, regardless of the argument passed to it. This illustrates an important point: syntax errors are always caught by the compiler. Although having to fix syntax errors is a nuisance, it is relatively easy. Most semantic errors are not caught by the compiler, so these are harder to spot and more difficult to fix. Compilers do offer some help detecting certain classes of semantic error, as you can see in the Tools section.","title":"Syntax vs. semantics"},{"location":"syntax_vs_semantics/#syntax-versus-semantics","text":"\"Syntax\" and \"semantics\" are two terms we will use often in this training. Most of you are probably familiar with these concepts, but just to make sure, let's define them briefly. The syntax of a programming language is its grammar, i.e., the rules the source code text must satisfy in order to be considered syntactically correct. Let's illustrate that with an example from natural language. The sentence \"the dog barks\" is syntactically correct, while \"the dog bark\" is not. Semantics on the other hand has to do with meaning, or interpretation. Again drawing on natural language examples, the sentence \"the dog spoke\" is syntactically correct, but the semantics is wrong. Obviously, dogs do not speak, except in fairy tales. The sentence \"the dog barks\" is both syntactically and semantically correct. The following code fragments would be syntactically incorrect. program syntax_error implicit none use, intrinsic :: iso_fortran_env, only : output_unit write (unit=output_unit, fmt='(A)') 'hello world' end program syntax_error This Fortran program has a syntax error that is dutifully reported by the gfortran compiler as: ~~~bash $ gfortran -c syntax_error.f90 syntax_error.f90:3.57: use, intrinsic :: iso_fortran_env, only : output_unit 1 syntax_error.f90:2.17: implicit none 2 Error: USE statement at (1) cannot follow IMPLICIT NONE statement at (2) The following Fortran function definition has a semantic error, although it is syntactically correct. ~~~~fortran integer function fac(n) implicit none integer, intent(IN) :: n integer :: i fac = 1 do i = 0, n fac = fac*i end do end function fac The code fragment above will be compiled without errors or even warnings, but the function will return zero when invoked, regardless of the argument passed to it. This illustrates an important point: syntax errors are always caught by the compiler. Although having to fix syntax errors is a nuisance, it is relatively easy. Most semantic errors are not caught by the compiler, so these are harder to spot and more difficult to fix. Compilers do offer some help detecting certain classes of semantic error, as you can see in the Tools section.","title":"Syntax versus semantics"},{"location":"version_control/","text":"Version control Version control is a very important aspect of software development. In short, it allows to answer the following questions. What was changed? When was it changed? Who made the change? Why was the change made? Using a version control system you can compare versions of your code, and if necessary, revert to a previous version. It is good practice to host you repositories on a service such as GitHub , GitLab or a hosting service provided by your organization. These environments all facilitate collaboration on software projects and make it easy to work in teams. You can find a PowerPoint presentation that introduces git and hosting services in the training materials repository for Version control with git .","title":"Version control"},{"location":"version_control/#version-control","text":"Version control is a very important aspect of software development. In short, it allows to answer the following questions. What was changed? When was it changed? Who made the change? Why was the change made? Using a version control system you can compare versions of your code, and if necessary, revert to a previous version. It is good practice to host you repositories on a service such as GitHub , GitLab or a hosting service provided by your organization. These environments all facilitate collaboration on software projects and make it easy to work in teams. You can find a PowerPoint presentation that introduces git and hosting services in the training materials repository for Version control with git .","title":"Version control"},{"location":"testing/code_coverage/","text":"Code coverage Introduction Unit tests are very useful to formulate fine-grained test to check the functionality of functions and methods. Unit tests check for edge and corner cases, but also for handling of error conditions such as exceptions being thrown. This is of course very useful in itself, but using a run of the complete unit test suite can also provide a good test to see whether all functions and methods are called, and all codes paths in the code get executed. Code coverage tools will instrument your code, run it, and provide feedback on regions of code that are not executed doing that run. Since we claim that code that is not tested is not correct, coverage provided by running all the unit tests should in fact be (close to) 100 %. If code coverage is insufficient, more unit tests should be added to the test suite. Best practices Maintaining code over long periods of time is quite expensive. The larger the code base, the more effort has to be spent on keeping code up to date. If some parts of the code are never used, that adds to this burden without return on investment. Updates are an issue as well, since some unused parts of the code may get out of sync with respect to the parts that are executed regularly. If someone starts using the abandoned part of the code, interesting bugs may creep into her code. Hence code that is not used is best removed from the code base. If you use a version control system and informative commit messages, it is quite easy to recover that code later when it is unexpectedly required. An important concern when writing tests for your software project is whether or not all branches in function, and indeed all functions are tested. Figuring out by hand whether that is the case is pretty hard for sizable projects. Fortunately, software tools are available for checking which parts of the code base are executed and which are not. For many programming languages, one has to resort to third party tools, but the compilers for C, C++ and Fortran support this out of the box. The first step in the workflow is to instrument the code with instructions to do the bookkeeping for reporting which lines of code have been executed. Compilers have options to do that automatically, so this can easily be incorporated into the build process by adding a make target specific for a code coverage build. The second step is to execute the software, so that a report is generated. In case you wonder how to run your code to get the most useful report, this depends on your goal. If you want to detect code that is likely not executed in applications, running a number of typical use cases are the best way to go. On the other hand, if you want to verify that you have a comprehensive set of unit tests, execute those. The third step is to inspect that report. Typically, you will get summary information, e.g., the percentage of the code covered in each file. In addition, each individual file can be inspected on a line by line basis. Lines that have not been executed are clearly marked, so that they are easy to spot. Finally, you decide to either weed out the lines if they are dead code, i.e., code that will never be executed in the context of your project, or to create additional unit tests if that code will be executed but is not tested yet. Since code coverage tests produce artifacts, it is best to add rules to remove these artifacts to your make file. This ensures you start with a clean slate when rebuilding. Code coverage assessment is an important tool for delivering good quality code, and goes hand in hand with unit testing and functional testing.","title":"Code coverage"},{"location":"testing/code_coverage/#code-coverage","text":"","title":"Code coverage"},{"location":"testing/code_coverage/#introduction","text":"Unit tests are very useful to formulate fine-grained test to check the functionality of functions and methods. Unit tests check for edge and corner cases, but also for handling of error conditions such as exceptions being thrown. This is of course very useful in itself, but using a run of the complete unit test suite can also provide a good test to see whether all functions and methods are called, and all codes paths in the code get executed. Code coverage tools will instrument your code, run it, and provide feedback on regions of code that are not executed doing that run. Since we claim that code that is not tested is not correct, coverage provided by running all the unit tests should in fact be (close to) 100 %. If code coverage is insufficient, more unit tests should be added to the test suite.","title":"Introduction"},{"location":"testing/code_coverage/#best-practices","text":"Maintaining code over long periods of time is quite expensive. The larger the code base, the more effort has to be spent on keeping code up to date. If some parts of the code are never used, that adds to this burden without return on investment. Updates are an issue as well, since some unused parts of the code may get out of sync with respect to the parts that are executed regularly. If someone starts using the abandoned part of the code, interesting bugs may creep into her code. Hence code that is not used is best removed from the code base. If you use a version control system and informative commit messages, it is quite easy to recover that code later when it is unexpectedly required. An important concern when writing tests for your software project is whether or not all branches in function, and indeed all functions are tested. Figuring out by hand whether that is the case is pretty hard for sizable projects. Fortunately, software tools are available for checking which parts of the code base are executed and which are not. For many programming languages, one has to resort to third party tools, but the compilers for C, C++ and Fortran support this out of the box. The first step in the workflow is to instrument the code with instructions to do the bookkeeping for reporting which lines of code have been executed. Compilers have options to do that automatically, so this can easily be incorporated into the build process by adding a make target specific for a code coverage build. The second step is to execute the software, so that a report is generated. In case you wonder how to run your code to get the most useful report, this depends on your goal. If you want to detect code that is likely not executed in applications, running a number of typical use cases are the best way to go. On the other hand, if you want to verify that you have a comprehensive set of unit tests, execute those. The third step is to inspect that report. Typically, you will get summary information, e.g., the percentage of the code covered in each file. In addition, each individual file can be inspected on a line by line basis. Lines that have not been executed are clearly marked, so that they are easy to spot. Finally, you decide to either weed out the lines if they are dead code, i.e., code that will never be executed in the context of your project, or to create additional unit tests if that code will be executed but is not tested yet. Since code coverage tests produce artifacts, it is best to add rules to remove these artifacts to your make file. This ensures you start with a clean slate when rebuilding. Code coverage assessment is an important tool for delivering good quality code, and goes hand in hand with unit testing and functional testing.","title":"Best practices"},{"location":"testing/functional_testing/","text":"Functional testing Introduction Unit testing is a great help during the development process. It will help us spot problems introduced by code changes immediately after they have been introduced. They help test the functionality at the level of individual functions and methods. However, to test an entire application that is, e.g., run from the command line with various parameters, unit tests are not really the right tool. The literature on software development refers to this type of testing as functional testing. Tests are more coarse grained, and are likely to take longer to run than you are comfortable with during an intensive edit/test/commit development cycle. Hence it is acceptable to run functional tests less often, for instance only when a feature has been added to the software, or a bug has been fixed that may have involved a considerable number of file edits and commits. We rely on unit testing to ensure that this process didn't break low-level integrity of the code. Ideally, functional testing is done automatically for a release with an online tool such as Travis CI or GitHub Actions for continuous integration. However, you can also do functional testing locally using, e.g., shunit2 or CTest . Functional testing can detect code defects that would go unnoticed by unit testing, so both testing strategies are complementing one another. Best practices Unit testing is an invaluable help for the developer since it catches bugs introduced when the code base changes. Tests can be executed easily and are run frequently. However, unit tests typically concentrate on the low level functionality of the software project. They test whether individual functions behave as expected. This is white box testing, since the tests are developed with access to the \"innards\" of the software under test. In some circumstances, this may be all that is required, e.g., when developing a relatively small or very focused library. In many cases though, unit testing is best supplemented by functional testing. The point of view of functional testing is opposite to that of unit testing since functional tests will focus on the application as a whole. Are the results for a sophisticated use case reproduced as expected? Does the application's user interface, command line interface (CLI), or graphical user interface (GUI) behave as expected? Are options handled as expected? This is often called black box testing since only the user interface is accessed. Functional testing can also be applied to third party applications that are part of a workflow. For instance, suppose that your application relies on the output of another application not developed by you. If the output format of that application changes from one version to the next, running a functional test will make clear whether there is an impact on your workflow, and you may fix problems by adapting your application. The best way to do functional testing is by using a continuous integration workflow. When the functional tests are run, first a container is prepared with the required operating system and software stack. Next, your software is built within the container, so that the environment is completely controlled. If the build succeeds, tests are run. A report is generated to show failures if they occur. Note that it is possible to set up a matrix of operating system versions and compiler versions to ensure that your code will build and executed cleanly on a wide range of software platforms. Both GitHub and Gitlab support continuous integration that can be used for functional testing. The question remains how to code the actual tests that will be executed by the continuous integration system. A convenient way is to reuse the unit test paradigm, but now on the level of the shell. In other words, the unit tests will be relatively short shell scripts that invoke your application using various parameters and input data, and verify the results. The shunit2 framework provides a nice framework for this purpose. It provides similar functionality as the unit testing frameworks for specific programming languages. However, from the point of view of the software project this is black box, rather than white box testing. The same concerns as for unit testing apply. For instance, it is important that the tests cover the use cases as well as possible. Here too, code coverage can be a great help to detect which application aspects are tested, and for which additional tests need to be implemented to improve the coverage. Examples CMake tests shunit2 tests CI for testing","title":"Functional testing"},{"location":"testing/functional_testing/#functional-testing","text":"","title":"Functional testing"},{"location":"testing/functional_testing/#introduction","text":"Unit testing is a great help during the development process. It will help us spot problems introduced by code changes immediately after they have been introduced. They help test the functionality at the level of individual functions and methods. However, to test an entire application that is, e.g., run from the command line with various parameters, unit tests are not really the right tool. The literature on software development refers to this type of testing as functional testing. Tests are more coarse grained, and are likely to take longer to run than you are comfortable with during an intensive edit/test/commit development cycle. Hence it is acceptable to run functional tests less often, for instance only when a feature has been added to the software, or a bug has been fixed that may have involved a considerable number of file edits and commits. We rely on unit testing to ensure that this process didn't break low-level integrity of the code. Ideally, functional testing is done automatically for a release with an online tool such as Travis CI or GitHub Actions for continuous integration. However, you can also do functional testing locally using, e.g., shunit2 or CTest . Functional testing can detect code defects that would go unnoticed by unit testing, so both testing strategies are complementing one another.","title":"Introduction"},{"location":"testing/functional_testing/#best-practices","text":"Unit testing is an invaluable help for the developer since it catches bugs introduced when the code base changes. Tests can be executed easily and are run frequently. However, unit tests typically concentrate on the low level functionality of the software project. They test whether individual functions behave as expected. This is white box testing, since the tests are developed with access to the \"innards\" of the software under test. In some circumstances, this may be all that is required, e.g., when developing a relatively small or very focused library. In many cases though, unit testing is best supplemented by functional testing. The point of view of functional testing is opposite to that of unit testing since functional tests will focus on the application as a whole. Are the results for a sophisticated use case reproduced as expected? Does the application's user interface, command line interface (CLI), or graphical user interface (GUI) behave as expected? Are options handled as expected? This is often called black box testing since only the user interface is accessed. Functional testing can also be applied to third party applications that are part of a workflow. For instance, suppose that your application relies on the output of another application not developed by you. If the output format of that application changes from one version to the next, running a functional test will make clear whether there is an impact on your workflow, and you may fix problems by adapting your application. The best way to do functional testing is by using a continuous integration workflow. When the functional tests are run, first a container is prepared with the required operating system and software stack. Next, your software is built within the container, so that the environment is completely controlled. If the build succeeds, tests are run. A report is generated to show failures if they occur. Note that it is possible to set up a matrix of operating system versions and compiler versions to ensure that your code will build and executed cleanly on a wide range of software platforms. Both GitHub and Gitlab support continuous integration that can be used for functional testing. The question remains how to code the actual tests that will be executed by the continuous integration system. A convenient way is to reuse the unit test paradigm, but now on the level of the shell. In other words, the unit tests will be relatively short shell scripts that invoke your application using various parameters and input data, and verify the results. The shunit2 framework provides a nice framework for this purpose. It provides similar functionality as the unit testing frameworks for specific programming languages. However, from the point of view of the software project this is black box, rather than white box testing. The same concerns as for unit testing apply. For instance, it is important that the tests cover the use cases as well as possible. Here too, code coverage can be a great help to detect which application aspects are tested, and for which additional tests need to be implemented to improve the coverage.","title":"Best practices"},{"location":"testing/functional_testing/#examples","text":"CMake tests shunit2 tests CI for testing","title":"Examples"},{"location":"testing/testing_as_experiments/","text":"Testing as scientific experiments If you are a scientist, or at least interested in the subject, you know about the scientific method. Simplified, the process works as follows: 1. You make a number of observations. 1. You formulate a hypothesis that has predictive power. 1. You design experiments to test predictions made by the hypothesis. 1. If an experiment succeeds, your confidence in the hypothesis increases. 1. However, if an experiment fails, you know for certain that there is a problem and you reformulate your hypothesis, possibly gathering some more observations to do so. It is worth pointing out that experiments are actually designed to disprove the hypothesis rather than to confirm it. A quote of the philosopher of science Karl Popper illustrates this. If we are uncritical we shall always find what we want: we shall look for, and find, confirmations, and we shall look away from, and not see, whatever might be dangerous to our pet theories. If you substitute \"pet theories\" in the above by \"pet implementations\", it is quite clear that the same maxims apply to software testing as to scientific research. Another quote from his famous book \"The logic of scientific discovery\" (1934) also translates well to software testing. ...no matter how many instances of white swans we may have observed, this does not justify the conclusion that all swans are white. Paraphrased, you can read that as \"... no matter how many tests your software may pass, it doesn't justify the conclusion that it is correct\".","title":"Testing as experiments"},{"location":"testing/testing_as_experiments/#testing-as-scientific-experiments","text":"If you are a scientist, or at least interested in the subject, you know about the scientific method. Simplified, the process works as follows: 1. You make a number of observations. 1. You formulate a hypothesis that has predictive power. 1. You design experiments to test predictions made by the hypothesis. 1. If an experiment succeeds, your confidence in the hypothesis increases. 1. However, if an experiment fails, you know for certain that there is a problem and you reformulate your hypothesis, possibly gathering some more observations to do so. It is worth pointing out that experiments are actually designed to disprove the hypothesis rather than to confirm it. A quote of the philosopher of science Karl Popper illustrates this. If we are uncritical we shall always find what we want: we shall look for, and find, confirmations, and we shall look away from, and not see, whatever might be dangerous to our pet theories. If you substitute \"pet theories\" in the above by \"pet implementations\", it is quite clear that the same maxims apply to software testing as to scientific research. Another quote from his famous book \"The logic of scientific discovery\" (1934) also translates well to software testing. ...no matter how many instances of white swans we may have observed, this does not justify the conclusion that all swans are white. Paraphrased, you can read that as \"... no matter how many tests your software may pass, it doesn't justify the conclusion that it is correct\".","title":"Testing as scientific experiments"},{"location":"testing/unit_testing/","text":"Unit testing Introduction Testing is obviously a way to ensure that at least part of the functionality behaves as expected. However, good tests can provide more than that, they can help ensure that changes to the code base don't introduce defects. A code base evolves dynamically, potentially over a long period of time. Adding new features to software is typically quite error prone, and might inadvertently break some use cases. In order to minimize this risk should have a sizable collection of tests available that check whether results are as expected, and be able to run those easily and frequently as part of your development cycle. Unit tests are an excellent approach. They consist of many small fragments of code that each test very specific aspects of the functionality of a library. Using frameworks such as pFUnit for Fortran, CUnit for C, Catch2 for C++, pytest for Python, or testthat for R will take care of the \"bookkeeping\" and ensure that running tests is effortless. Best practices Very often code defects are introduced while the code evolves to implement new features or even to fix emerging problems. Without proper testing these new bugs may go unnoticed for a while, even until after the release of a new version of a software project. Hence it is good practice to have a set of tests at hand for regression testing, i.e., testing whether what used to work correctly, still does. It is a very important aspect of a software project with a non-trivial life-cycle. Testing code by running an application and visually inspecting the results is very error-prone. It is all too easy to miss a problem when the output is fairly long, as would be the case for most software projects. Hence a different approach should be taken. Another consideration when planning tests is that they should be easy to run and should complete in a reasonably short time. Such tests offer no excuses to delay running them or to not run them frequently. If those tests can be part of the software build process, and hence automated, that is an additional benefit. From the output of a test failure, it should be very clear what the issue is. If we have tests that check a single concern, or a single use case, this will help to pinpoint the failure's causes much more easily. Fortunately, all these criteria are met by the unit testing paradigm, popularized by the extreme programming developed by Kent Beck and Ron Jeffries around 1998. Quoting from \"The art of unit testing\", Kent Beck defines a unit test as A unit test is an automated piece of code that invokes a unit of work in the system and then checks a single assumption about the behavior of that unit of work. Many implementations of unit testing frameworks exist, often multiple for the same programming language. They provide a framework that takes care of the scaffolding, i.e., the main function, to run the test, the formatting of the tests output and messages, and summary reporting. As a programmer, you simply implement functions that test for specific features of your software, usually at the level of individual functions. The framework provides functions to compare expected to computed values, the result of Boolean expressions, whether exceptions have been thrown (if expected), and so on. Once the test code is written, building and running it is very easy, and can be added as a target to a make file. What to test? You will typically write unit tests that compare the return value of a function to an expected value. Good tests not only check for normal cases, but also for edge or corner cases. By way of example, consider the factorial function. One would of course write tests to verify that its return value for 3 is 6, or for 5 is 120, but the cases where it is easy to make mistakes should also be tested, in this example the input values 0 and 1 (edge cases). If the code under tests has branches, i.e., conditional statements, we should make sure that there is at least one test for each branch. The same applies to iteration statements when the number of iterations is computed, there should be tests in case no iterations have to be executed, a single iteration or multiple iterations. Another important aspect to test for is whether a function throws an exception or generates an error when it is supposed to, since this is the function's declared behavior. For the factorial example, we should verify that an error occurs when you pass to it a strictly negative argument value. Apart from providing some confidence that changes don't break our software, writing tests alongside code will actually prevent bugs, since you should really think about the behavior of your code when writing tests. It is quite probable that while doing so, you will catch bugs without even executing the unit tests. This idea is taken to the extreme in Test Driven Development (TDD). A requirement is translated into a number of tests, and only when these are implemented, the actual functions are developed. This approach is kind of satisfying: at first all tests fail, since there is a trivial implementation only. As the development progresses, and the implementation gradually nears completion, more and more tests succeed. How to test? Note that unit tests should be simple, i.e., a unit test check one particular aspect, so that failure is easy to map to its cause. This means that functions implementing unit tests are typically very short, but that we may have quite many of them. Another very important issue to keep in mind is that tests should be independent. You should be able to run them in any order without altering the results. This implies that unit tests have to be free of side effects. To summarize the characteristics of a good unit test: it is specific: failure of a test localizes a defect in a small fragment of code; it is orthogonal to other unit tests: unit tests should be independent of one another. The collection of all tests for your software project should be complete. There should be tests for all functions, but also for all code paths through your code. An aspect that is often forgotten is that you should also test for expected failure. Is an exception actually thrown, or does the exit status of a function reflect a problem? When implementing a new feature or making a change, you should of course develop tests specific to that addition or modification. However, it is not enough to simply run those new tests. Even if they fail, your code might still be broken, since an addition or a modification might introduce a bug for existing code. This situation is called a regression, since the code was correct, but due to changes, it no longer is. Hence it is important to run all tests, to ensure that there are no regressions. This practice is referred to as regression testing. If unit testing is done well, regression testing is always performed. Of course, many tests require a context to run in. Variables and data structures must have been initialized, a file should be present, and so on. Unit testing frameworks provide setup and teardown functions that can be run before and after each individual test respectively to set, or clean up the stage. These temporary artifacts are often referred to as fixtures. Unit testing frameworks typically also provide the means to group tests into so called suites, and run setup and teardown functions before running the first and after running the last test of that suite. Units tests for large projects are typically organized into multiple suites that deal with specific subsets of the code or functionality. Below you can see when various setup and teardown functions are called when test suites are executed. Examples Examples of unit testing can be found in the following repositories. Cunit for C Catch2 examples for C++ pFunit examples for Fortran pytest examples for Python","title":"Unit testing"},{"location":"testing/unit_testing/#unit-testing","text":"","title":"Unit testing"},{"location":"testing/unit_testing/#introduction","text":"Testing is obviously a way to ensure that at least part of the functionality behaves as expected. However, good tests can provide more than that, they can help ensure that changes to the code base don't introduce defects. A code base evolves dynamically, potentially over a long period of time. Adding new features to software is typically quite error prone, and might inadvertently break some use cases. In order to minimize this risk should have a sizable collection of tests available that check whether results are as expected, and be able to run those easily and frequently as part of your development cycle. Unit tests are an excellent approach. They consist of many small fragments of code that each test very specific aspects of the functionality of a library. Using frameworks such as pFUnit for Fortran, CUnit for C, Catch2 for C++, pytest for Python, or testthat for R will take care of the \"bookkeeping\" and ensure that running tests is effortless.","title":"Introduction"},{"location":"testing/unit_testing/#best-practices","text":"Very often code defects are introduced while the code evolves to implement new features or even to fix emerging problems. Without proper testing these new bugs may go unnoticed for a while, even until after the release of a new version of a software project. Hence it is good practice to have a set of tests at hand for regression testing, i.e., testing whether what used to work correctly, still does. It is a very important aspect of a software project with a non-trivial life-cycle. Testing code by running an application and visually inspecting the results is very error-prone. It is all too easy to miss a problem when the output is fairly long, as would be the case for most software projects. Hence a different approach should be taken. Another consideration when planning tests is that they should be easy to run and should complete in a reasonably short time. Such tests offer no excuses to delay running them or to not run them frequently. If those tests can be part of the software build process, and hence automated, that is an additional benefit. From the output of a test failure, it should be very clear what the issue is. If we have tests that check a single concern, or a single use case, this will help to pinpoint the failure's causes much more easily. Fortunately, all these criteria are met by the unit testing paradigm, popularized by the extreme programming developed by Kent Beck and Ron Jeffries around 1998. Quoting from \"The art of unit testing\", Kent Beck defines a unit test as A unit test is an automated piece of code that invokes a unit of work in the system and then checks a single assumption about the behavior of that unit of work. Many implementations of unit testing frameworks exist, often multiple for the same programming language. They provide a framework that takes care of the scaffolding, i.e., the main function, to run the test, the formatting of the tests output and messages, and summary reporting. As a programmer, you simply implement functions that test for specific features of your software, usually at the level of individual functions. The framework provides functions to compare expected to computed values, the result of Boolean expressions, whether exceptions have been thrown (if expected), and so on. Once the test code is written, building and running it is very easy, and can be added as a target to a make file.","title":"Best practices"},{"location":"testing/unit_testing/#what-to-test","text":"You will typically write unit tests that compare the return value of a function to an expected value. Good tests not only check for normal cases, but also for edge or corner cases. By way of example, consider the factorial function. One would of course write tests to verify that its return value for 3 is 6, or for 5 is 120, but the cases where it is easy to make mistakes should also be tested, in this example the input values 0 and 1 (edge cases). If the code under tests has branches, i.e., conditional statements, we should make sure that there is at least one test for each branch. The same applies to iteration statements when the number of iterations is computed, there should be tests in case no iterations have to be executed, a single iteration or multiple iterations. Another important aspect to test for is whether a function throws an exception or generates an error when it is supposed to, since this is the function's declared behavior. For the factorial example, we should verify that an error occurs when you pass to it a strictly negative argument value. Apart from providing some confidence that changes don't break our software, writing tests alongside code will actually prevent bugs, since you should really think about the behavior of your code when writing tests. It is quite probable that while doing so, you will catch bugs without even executing the unit tests. This idea is taken to the extreme in Test Driven Development (TDD). A requirement is translated into a number of tests, and only when these are implemented, the actual functions are developed. This approach is kind of satisfying: at first all tests fail, since there is a trivial implementation only. As the development progresses, and the implementation gradually nears completion, more and more tests succeed.","title":"What to test?"},{"location":"testing/unit_testing/#how-to-test","text":"Note that unit tests should be simple, i.e., a unit test check one particular aspect, so that failure is easy to map to its cause. This means that functions implementing unit tests are typically very short, but that we may have quite many of them. Another very important issue to keep in mind is that tests should be independent. You should be able to run them in any order without altering the results. This implies that unit tests have to be free of side effects. To summarize the characteristics of a good unit test: it is specific: failure of a test localizes a defect in a small fragment of code; it is orthogonal to other unit tests: unit tests should be independent of one another. The collection of all tests for your software project should be complete. There should be tests for all functions, but also for all code paths through your code. An aspect that is often forgotten is that you should also test for expected failure. Is an exception actually thrown, or does the exit status of a function reflect a problem? When implementing a new feature or making a change, you should of course develop tests specific to that addition or modification. However, it is not enough to simply run those new tests. Even if they fail, your code might still be broken, since an addition or a modification might introduce a bug for existing code. This situation is called a regression, since the code was correct, but due to changes, it no longer is. Hence it is important to run all tests, to ensure that there are no regressions. This practice is referred to as regression testing. If unit testing is done well, regression testing is always performed. Of course, many tests require a context to run in. Variables and data structures must have been initialized, a file should be present, and so on. Unit testing frameworks provide setup and teardown functions that can be run before and after each individual test respectively to set, or clean up the stage. These temporary artifacts are often referred to as fixtures. Unit testing frameworks typically also provide the means to group tests into so called suites, and run setup and teardown functions before running the first and after running the last test of that suite. Units tests for large projects are typically organized into multiple suites that deal with specific subsets of the code or functionality. Below you can see when various setup and teardown functions are called when test suites are executed.","title":"How to test?"},{"location":"testing/unit_testing/#examples","text":"Examples of unit testing can be found in the following repositories. Cunit for C Catch2 examples for C++ pFunit examples for Fortran pytest examples for Python","title":"Examples"},{"location":"tools/","text":"Tools There are many useful development/software engineering tools available. This is a list of tools we find useful, but it doesn't aim to be exhaustive. What is it? General tools : language-agnostic tools and services. C++ tools : tools specific for C++ development. Python tools : tools specific for Python development. R tools : tools specific for R development.","title":"Tools"},{"location":"tools/#tools","text":"There are many useful development/software engineering tools available. This is a list of tools we find useful, but it doesn't aim to be exhaustive.","title":"Tools"},{"location":"tools/#what-is-it","text":"General tools : language-agnostic tools and services. C++ tools : tools specific for C++ development. Python tools : tools specific for Python development. R tools : tools specific for R development.","title":"What is it?"},{"location":"tools/C-plus-plus/","text":"Tools for C++ programming Code formatting clang-format : code formatter is part of the Clang Tools. Linting and static analysis clang-tidy : linter, part of Extra Clang Tools. cppcheck : linter for C and C++. Package managers Conan : package manager for C and C++. vspkg : package manager for C and C++. Testing Catch2 : C++ unit testing framework that also supports Behavior-Driven Development (BDD).","title":"C++"},{"location":"tools/C-plus-plus/#tools-for-c-programming","text":"","title":"Tools for C++ programming"},{"location":"tools/C-plus-plus/#code-formatting","text":"clang-format : code formatter is part of the Clang Tools.","title":"Code formatting"},{"location":"tools/C-plus-plus/#linting-and-static-analysis","text":"clang-tidy : linter, part of Extra Clang Tools. cppcheck : linter for C and C++.","title":"Linting and static analysis"},{"location":"tools/C-plus-plus/#package-managers","text":"Conan : package manager for C and C++. vspkg : package manager for C and C++.","title":"Package managers"},{"location":"tools/C-plus-plus/#testing","text":"Catch2 : C++ unit testing framework that also supports Behavior-Driven Development (BDD).","title":"Testing"},{"location":"tools/C/","text":"Tools for C programming Code formatting clang-format : code formatter is part of the Clang Tools. Linting and static analysis clang-tidy : linter, part of Extra Clang Tools. cppcheck : linter for C and C++. Package managers Conan : package manager for C and C++. vspkg : package manager for C and C++. Testing CUnit : unit testing framework for C.","title":"C"},{"location":"tools/C/#tools-for-c-programming","text":"","title":"Tools for C programming"},{"location":"tools/C/#code-formatting","text":"clang-format : code formatter is part of the Clang Tools.","title":"Code formatting"},{"location":"tools/C/#linting-and-static-analysis","text":"clang-tidy : linter, part of Extra Clang Tools. cppcheck : linter for C and C++.","title":"Linting and static analysis"},{"location":"tools/C/#package-managers","text":"Conan : package manager for C and C++. vspkg : package manager for C and C++.","title":"Package managers"},{"location":"tools/C/#testing","text":"CUnit : unit testing framework for C.","title":"Testing"},{"location":"tools/Fortran/","text":"Tools for Fortran programming Testing pFUnit : is a very rich framework for developing unit tests. Build tools FPM : Fortran Package Manager (FPM) helps you create Fortran software project, initializing the project directory with the appropriate directories, configuration files and such. It also supports managing dependencies, running your code and tests.","title":"Fortran"},{"location":"tools/Fortran/#tools-for-fortran-programming","text":"","title":"Tools for Fortran programming"},{"location":"tools/Fortran/#testing","text":"pFUnit : is a very rich framework for developing unit tests.","title":"Testing"},{"location":"tools/Fortran/#build-tools","text":"FPM : Fortran Package Manager (FPM) helps you create Fortran software project, initializing the project directory with the appropriate directories, configuration files and such. It also supports managing dependencies, running your code and tests.","title":"Build tools"},{"location":"tools/Python/","text":"Tools for Python programming Code formatting Black : code formatter for Python (very opinionated). Linting and static analysis flake8 : tool to enforce adherence to the Python style guide. mypy : if your code has type annotations, mypy will check for correctness. pylint : static code analyzer that also checks for documentation and style. Package managers miniconda : Python package manager. mamba : Python package manager. Poetry : environment management, build and deployment of Python projects. Testing pytest : unit testing framework. Build tools Poetry : environment management, build and deployment of Python projects.","title":"Python"},{"location":"tools/Python/#tools-for-python-programming","text":"","title":"Tools for Python programming"},{"location":"tools/Python/#code-formatting","text":"Black : code formatter for Python (very opinionated).","title":"Code formatting"},{"location":"tools/Python/#linting-and-static-analysis","text":"flake8 : tool to enforce adherence to the Python style guide. mypy : if your code has type annotations, mypy will check for correctness. pylint : static code analyzer that also checks for documentation and style.","title":"Linting and static analysis"},{"location":"tools/Python/#package-managers","text":"miniconda : Python package manager. mamba : Python package manager. Poetry : environment management, build and deployment of Python projects.","title":"Package managers"},{"location":"tools/Python/#testing","text":"pytest : unit testing framework.","title":"Testing"},{"location":"tools/Python/#build-tools","text":"Poetry : environment management, build and deployment of Python projects.","title":"Build tools"},{"location":"tools/R/","text":"Tools for R programmers Testing testthat : testing framework for R. Software deployment usethis : takes care of repetitive task associated with package setup and R development more generally.","title":"R"},{"location":"tools/R/#tools-for-r-programmers","text":"","title":"Tools for R programmers"},{"location":"tools/R/#testing","text":"testthat : testing framework for R.","title":"Testing"},{"location":"tools/R/#software-deployment","text":"usethis : takes care of repetitive task associated with package setup and R development more generally.","title":"Software deployment"},{"location":"tools/general/","text":"Programming language-agnostic tools This is a list of tools that are not specific to a specific programming language. Version control git : version control software. SmartGit : nice git GUI application that is cross-platform (Windows/MacOS/Linux) and can be used with multiple hosting services. DVC : Data Version Control, tool to manage versions of your data alongside your code. Using this tool avoid having to commit large files into a git repository for which it is not intended. GitHub hosting platform. GitLab hosting platform. Build tools Some build tools can be used for many programming languages. Build tools let you specify how to build your applications and libraries, but often also how to test and package them. CMake : cross-platform build tool that can be used to build and install C/C++/Fortran libraries and applications. A number of examples of using CMake in various scenarios can be found in the repository on CMake use cases . Documentation In this session, we will discuss two tools for creating attractive documentation, Doxygen and MkDocs. The former is best suited for reference guides, while the latter is excellent for tutorial-style material. Doxygen : some programming languages such as Java and Python provide support for documentation as part of their specification. The languages we use most frequently in an HPC context, C, C++, and Fortran, have no such provisions. However, Doxygen generates reference documentation out of comment blocks for a wide variety of programming languages, including those of interest to us. This documentation is fully hyperlinked. For instance, clicking the type of a function's argument will bring you to the type's documentation. MkDocs : this is a very convenient tool for generating nice looking documentation that can be viewed standalone as HTML pages, or that can be served from the ReadTheDocs service. It automatically generates a navigation panel and adds search functionality. You can also define a GitHub trigger that will automatically push your project's documentation to Read the Docs each time you do a release. Documentation of previous software versions remain available. In that scenario, MkDocs will provide useful previews before you make a release of your code project. Sphinx : this is another tool to generate documentation. It can generate API reference documentation for Python, and tutorial style documentation in general. The resulting documentation can be hosted on ReadTheDocs or GitHub pages. ReadTheDocs : a hosting service for documentation. It supports both MkDocs and Sphinx. Documentation can be fetched from a GitHub repository and (re)built. This can be automized and set to be triggered by, e.g., a merge into main. GitHub pages : you can activate pages for any GitHub repository. This will create a website that you can use to host the documentation for your project to make it available to your group or even to every user of your software. The documentaiton can be genreated using a GitHub Action triggered by, for instance, a merge into the main branch. The repository that hosts this information is an example of that. Testing Some testing tools are generic and can be used to do functional testing for application developed in any programming language. shunit2 framework : use Bash scripts to perform functional testing as unit tests. CTest : CTest is part of CMake and lets you do functional testing as part of the build process. Licensing Selecting an appropirate license is not trivial. Website that tries to guide you through the process. Attribution Zenodo : website to request a Digital Object Identifier (DOI) for your version control repositories.","title":"General tools"},{"location":"tools/general/#programming-language-agnostic-tools","text":"This is a list of tools that are not specific to a specific programming language.","title":"Programming language-agnostic tools"},{"location":"tools/general/#version-control","text":"git : version control software. SmartGit : nice git GUI application that is cross-platform (Windows/MacOS/Linux) and can be used with multiple hosting services. DVC : Data Version Control, tool to manage versions of your data alongside your code. Using this tool avoid having to commit large files into a git repository for which it is not intended. GitHub hosting platform. GitLab hosting platform.","title":"Version control"},{"location":"tools/general/#build-tools","text":"Some build tools can be used for many programming languages. Build tools let you specify how to build your applications and libraries, but often also how to test and package them. CMake : cross-platform build tool that can be used to build and install C/C++/Fortran libraries and applications. A number of examples of using CMake in various scenarios can be found in the repository on CMake use cases .","title":"Build tools"},{"location":"tools/general/#documentation","text":"In this session, we will discuss two tools for creating attractive documentation, Doxygen and MkDocs. The former is best suited for reference guides, while the latter is excellent for tutorial-style material. Doxygen : some programming languages such as Java and Python provide support for documentation as part of their specification. The languages we use most frequently in an HPC context, C, C++, and Fortran, have no such provisions. However, Doxygen generates reference documentation out of comment blocks for a wide variety of programming languages, including those of interest to us. This documentation is fully hyperlinked. For instance, clicking the type of a function's argument will bring you to the type's documentation. MkDocs : this is a very convenient tool for generating nice looking documentation that can be viewed standalone as HTML pages, or that can be served from the ReadTheDocs service. It automatically generates a navigation panel and adds search functionality. You can also define a GitHub trigger that will automatically push your project's documentation to Read the Docs each time you do a release. Documentation of previous software versions remain available. In that scenario, MkDocs will provide useful previews before you make a release of your code project. Sphinx : this is another tool to generate documentation. It can generate API reference documentation for Python, and tutorial style documentation in general. The resulting documentation can be hosted on ReadTheDocs or GitHub pages. ReadTheDocs : a hosting service for documentation. It supports both MkDocs and Sphinx. Documentation can be fetched from a GitHub repository and (re)built. This can be automized and set to be triggered by, e.g., a merge into main. GitHub pages : you can activate pages for any GitHub repository. This will create a website that you can use to host the documentation for your project to make it available to your group or even to every user of your software. The documentaiton can be genreated using a GitHub Action triggered by, for instance, a merge into the main branch. The repository that hosts this information is an example of that.","title":"Documentation"},{"location":"tools/general/#testing","text":"Some testing tools are generic and can be used to do functional testing for application developed in any programming language. shunit2 framework : use Bash scripts to perform functional testing as unit tests. CTest : CTest is part of CMake and lets you do functional testing as part of the build process.","title":"Testing"},{"location":"tools/general/#licensing","text":"Selecting an appropirate license is not trivial. Website that tries to guide you through the process.","title":"Licensing"},{"location":"tools/general/#attribution","text":"Zenodo : website to request a Digital Object Identifier (DOI) for your version control repositories.","title":"Attribution"}]}